{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269790d1",
   "metadata": {},
   "source": [
    "# Assignments2: Deep N-grams"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4907be3",
   "metadata": {},
   "source": [
    "Overview\n",
    "Part 1: Importing the Data\n",
    "    1.1 Loading in the data\n",
    "    1.2 Convert a line to tensor\n",
    "    Exercise 01\n",
    "    1.3 Batch generator\n",
    "    Exercise 02\n",
    "    1.4 Repeating Batch generator\n",
    "Part 2: Defining the GRU model\n",
    "    Exercise 03\n",
    "Part 3: Training\n",
    "    3.1 Training the Model\n",
    "    Exercise 04\n",
    "Part 4: Evaluation\n",
    "    4.1 Evaluating using the deep nets\n",
    "    Exercise 05\n",
    "Part 5: Generating the language with your own model\n",
    "    Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f773b",
   "metadata": {},
   "source": [
    "# some changes in the new trax; eval_tasks=[eval_task] !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90570eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33072bb2",
   "metadata": {},
   "source": [
    "### Part1: Importing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b1e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ord function to index unique character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fdb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'data/'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "for filename in os.listdir(dirname):\n",
    "    with open(os.path.join(dirname, filename)) as files:\n",
    "        for line in files:\n",
    "            # remove leading and trailing whitespace\n",
    "            pure_line = line.strip()\n",
    "            \n",
    "            # if pure_line is not the empty string,\n",
    "            if pure_line:\n",
    "                # append it to the list\n",
    "                lines.append(pure_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153c47fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0 JULIUS CAESAR\n",
      "Sample line at position 999 Of your good pleasure? If it be no more,\n"
     ]
    }
   ],
   "source": [
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e1660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e0144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0 julius caesar\n",
      "Sample line at position 999 of your good pleasure? if it be no more,\n"
     ]
    }
   ],
   "source": [
    "# go through each line\n",
    "for i, line in enumerate(lines):\n",
    "    # convert to all lowercase\n",
    "    lines[i] = line.lower()\n",
    "\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065c07c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for training: 124097\n",
      "Number of lines for validation: 1000\n"
     ]
    }
   ],
   "source": [
    "eval_lines = lines[-1000:] # Create a holdout validation set\n",
    "lines = lines[:-1000] # Leave the rest for training\n",
    "\n",
    "print(f\"Number of lines for training: {len(lines)}\")\n",
    "print(f\"Number of lines for validation: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec818a3c",
   "metadata": {},
   "source": [
    "### Convert a line to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71721642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord('a'): 97\n",
      "ord('b'): 98\n",
      "ord('c'): 99\n",
      "ord(' '): 32\n",
      "ord('x'): 120\n",
      "ord('y'): 121\n",
      "ord('z'): 122\n",
      "ord('1'): 49\n",
      "ord('2'): 50\n",
      "ord('3'): 51\n"
     ]
    }
   ],
   "source": [
    "# View the unique unicode integer associated with each character\n",
    "print(f\"ord('a'): {ord('a')}\")\n",
    "print(f\"ord('b'): {ord('b')}\")\n",
    "print(f\"ord('c'): {ord('c')}\")\n",
    "print(f\"ord(' '): {ord(' ')}\")\n",
    "print(f\"ord('x'): {ord('x')}\")\n",
    "print(f\"ord('y'): {ord('y')}\")\n",
    "print(f\"ord('z'): {ord('z')}\")\n",
    "print(f\"ord('1'): {ord('1')}\")\n",
    "print(f\"ord('2'): {ord('2')}\")\n",
    "print(f\"ord('3'): {ord('3')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aeb8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: line_to_tensor\n",
    "def line_to_tensor(line, EOS_int=1):\n",
    "    \"\"\"Turns a line of text into a tensor\n",
    "\n",
    "    Args:\n",
    "        line (str): A single line of text.\n",
    "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of integers (unicode values) for the characters in the `line`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the tensor as an empty list\n",
    "    tensor = []\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # for each character:\n",
    "    for c in line:\n",
    "        \n",
    "        # convert to unicode int\n",
    "        c_int = ord(c)\n",
    "        \n",
    "        # append the unicode integer to the tensor list\n",
    "        tensor.append(c_int)\n",
    "    \n",
    "    # include the end-of-sentence integer\n",
    "    tensor.append(EOS_int)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d173f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 98, 99, 32, 120, 121, 122, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your output\n",
    "line_to_tensor('abc xyz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9fa7b",
   "metadata": {},
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9e4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can iterate like: next(data_generator)\n",
    "\n",
    "# The batch is a tuple of three parts: inputs, targets, mask. The inputs and targets are identical. The second column will be used to evaluate your predictions. \n",
    "# Mask is 1 for non-padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a14362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
    "        max_length (int): maximum length of the output tensor.\n",
    "        NOTE: max_length includes the end-of-sentence character that will be added\n",
    "                to the tensor.  \n",
    "                Keep in mind that the length of the tensor is always 1 + the length\n",
    "                of the original line of characters.\n",
    "        data_lines (list): list of the sentences to group into batches.\n",
    "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
    "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
    "\n",
    "    Yields:\n",
    "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
    "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
    "    \"\"\"\n",
    "    # initialize the index that points to the current position in the lines index array\n",
    "    index = 0\n",
    "    \n",
    "    # initialize the list that will contain the current batch\n",
    "    cur_batch = []\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(data_lines)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle line indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    while True:\n",
    "        \n",
    "        # if the index is greater or equal than to the number of lines in data_lines\n",
    "        if index >= num_lines:\n",
    "            # then reset the index to 0\n",
    "            index = 0\n",
    "            # shuffle line indexes if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(lines_index)\n",
    "            \n",
    "        # get a line at the `lines_index[index]` position in data_lines\n",
    "        line = data_lines[lines_index[index]]\n",
    "        \n",
    "        # if the length of the line is less than max_length\n",
    "        if len(line) < max_length:\n",
    "            # append the line to the current batch\n",
    "            cur_batch.append(line)\n",
    "            \n",
    "        # increment the index by one\n",
    "        index += 1\n",
    "        \n",
    "        # if the current batch is now equal to the desired batch size\n",
    "        if len(cur_batch) == batch_size:\n",
    "            \n",
    "            batch = []\n",
    "            mask = []\n",
    "            \n",
    "            # go through each line (li) in cur_batch\n",
    "            for li in cur_batch:\n",
    "                # convert the line (li) to a tensor of integers\n",
    "                tensor = line_to_tensor(li)\n",
    "                \n",
    "                # Create a list of zeros to represent the padding\n",
    "                # so that the tensor plus padding will have length `max_length`\n",
    "                pad = [0] * (max_length - len(tensor))\n",
    "                \n",
    "                # combine the tensor plus pad\n",
    "                tensor_pad = tensor + pad\n",
    "                \n",
    "                # append the padded tensor to the batch\n",
    "                batch.append(tensor_pad)\n",
    "\n",
    "                # A mask for  tensor_pad is 1 wherever tensor_pad is not\n",
    "                # 0 and 0 wherever tensor_pad is 0, i.e. if tensor_pad is\n",
    "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
    "                # [1, 1, 1, 0, 0, 0]\n",
    "                # Hint: Use a list comprehension for this\n",
    "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
    "                mask.append(example_mask)\n",
    "               \n",
    "            # convert the batch (data type list) to a trax's numpy array\n",
    "            batch_np_arr = np.array(batch)\n",
    "            mask_np_arr = np.array(mask)\n",
    "            \n",
    "            ### END CODE HERE ##\n",
    "            \n",
    "            # Yield two copies of the batch and mask.\n",
    "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
    "            \n",
    "            # reset the current batch to an empty list\n",
    "            cur_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c87de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lines_index)\n",
    "# print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fff26231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
       "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
       " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
       "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
       " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out your data generator\n",
    "tmp_lines = ['12345678901', #length 11\n",
    "             '123456789', # length 9\n",
    "             '234567890', # length 9\n",
    "             '345678901'] # length 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = data_generator(batch_size=2, \n",
    "                              max_length=10, \n",
    "                              data_lines=tmp_lines,\n",
    "                              shuffle=False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae34d56",
   "metadata": {},
   "source": [
    "### Repeating Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "infinite_data_generator = itertools.cycle(\n",
    "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca578fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
    "print(len(ten_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58bb80",
   "metadata": {},
   "source": [
    "### Part2: Deinfing the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "437d3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: GRULM\n",
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    \"\"\"Returns a GRU language model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
    "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
    "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    model = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "      tl.Dense(n_units=vocab_size), # Dense layer\n",
    "      tl.LogSoftmax()  # Log Softmax\n",
    "    )\n",
    "    ### END CODE HERE ###\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17597584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# testing your model\n",
    "model = GRULM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175bda9",
   "metadata": {},
   "source": [
    "### Part3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dff487a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train the model: trax.supervised.training.TrainTask\n",
    "# to evalutate a model: trax.supervised.training.EvalTask\n",
    "# to tie together: trax.supervised.training.Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "254ecb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "820aaca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many lines and how many epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0afca7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of used lines from the dataset: 25826\n",
      "Batch size (a power of 2): 32\n",
      "Number of steps to cover one epoch: 807\n"
     ]
    }
   ],
   "source": [
    "def n_used_lines(lines, max_length):\n",
    "    '''\n",
    "    Args: \n",
    "    lines: all lines of text an array of lines\n",
    "    max_length - max_length of a line in order to be considered an int\n",
    "    output_dir - folder to save your file an int\n",
    "    Return:\n",
    "    number of efective examples\n",
    "    '''\n",
    "\n",
    "    n_lines = 0\n",
    "    for l in lines:\n",
    "        if len(l) <= max_length:\n",
    "            n_lines += 1\n",
    "    return n_lines\n",
    "\n",
    "num_used_lines = n_used_lines(lines, 32)\n",
    "print('Number of used lines from the dataset:', num_used_lines)\n",
    "print('Batch size (a power of 2):', int(batch_size))\n",
    "steps_per_epoch = int(num_used_lines/batch_size)\n",
    "print('Number of steps to cover one epoch:', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ab66b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): \n",
    "    \"\"\"Function that trains the model\n",
    "\n",
    "    Args:\n",
    "        model (trax.layers.combinators.Serial): GRU model.\n",
    "        data_generator (function): Data generator function.\n",
    "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
    "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
    "        lines (list, optional): List of lines to use for training. Defaults to lines.\n",
    "        eval_lines (list, optional): List of lines to use for evaluation. Defaults to eval_lines.\n",
    "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
    "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "   \n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator, # Use infinite train data generator\n",
    "        loss_layer=tl.CrossEntropyLoss(),   # Don't forget to instantiate this object\n",
    "        optimizer=trax.optimizers.Adam(0.0005),     # Don't forget to add the learning rate parameter\n",
    "        n_steps_per_checkpoint=500\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,    # Use infinite eval data generator\n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], # Don't forget to instantiate these objects\n",
    "        n_eval_batches=3      # For better evaluation accuracy in reasonable time\n",
    "    )\n",
    "    \n",
    "    training_loop = training.Loop(model,\n",
    "                                  train_task,\n",
    "                                  eval_tasks=[eval_task],\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # We return this because it contains a handle to the model, which has the weights etc.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "196d1849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not manage to match shapes in model for all checkpoint weights.\n",
      "  Not inserted tensor of shape (9087, 256)\n",
      "  Tensor in that place has shape: (256, 512)\n",
      "  Not inserted tensor of shape (9087, 256)\n",
      "  Tensor in that place has shape: (256, 512)\n",
      "  Not inserted tensor of shape (256, 2)\n",
      "  Tensor in that place has shape: (1024, 1024)\n",
      "  Not inserted tensor of shape (256, 2)\n",
      "  Tensor in that place has shape: (1024, 1024)\n",
      "  Not inserted tensor of shape (2,)\n",
      "  Tensor in that place has shape: (1024,)\n",
      "  Not inserted tensor of shape (2,)\n",
      "  Tensor in that place has shape: (1024,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d92073c6bf76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRULM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-a74762678d27>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_generator, batch_size, max_length, lines, eval_lines, n_steps, output_dir)\u001b[0m\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     training_loop = training.Loop(model,\n\u001b[0m\u001b[1;32m     43\u001b[0m                                   \u001b[0mtrain_task\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                   \u001b[0meval_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/supervised/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tasks, eval_model, eval_tasks, output_dir, checkpoint_at, checkpoint_low_metric, checkpoint_high_metric, permanent_checkpoint_at, eval_at, which_task, n_devices, random_seed, loss_chunk_size, use_memory_efficient_trainer, adasum, callbacks)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;31m# Load checkpoint if it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;31m# Prepare eval components.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/supervised/training.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(self, directory, filename)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slots_per_task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslots\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer_per_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slots_per_task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m       matched_flat_slots = _match_by_shape(\n\u001b[0m\u001b[1;32m    942\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_bits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten_and_remove_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m           _flatten_and_remove_empty(slots))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/supervised/training.py\u001b[0m in \u001b[0;36m_match_by_shape\u001b[0;34m(full, partial)\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmodel_weight_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpartial_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0m_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  Tensor in that place has shape: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_weight_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\n",
    "training_loop = train_model(GRULM(), data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54f251",
   "metadata": {},
   "source": [
    "# Bug:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50e637e2",
   "metadata": {},
   "source": [
    "Did not manage to match shapes in model for all checkpoint weights.\n",
    "  Not inserted tensor of shape (9087, 256)\n",
    "  Tensor in that place has shape: (256, 512)\n",
    "  Not inserted tensor of shape (9087, 256)\n",
    "  Tensor in that place has shape: (256, 512)\n",
    "  Not inserted tensor of shape (256, 2)\n",
    "  Tensor in that place has shape: (1024, 1024)\n",
    "  Not inserted tensor of shape (256, 2)\n",
    "  Tensor in that place has shape: (1024, 1024)\n",
    "  Not inserted tensor of shape (2,)\n",
    "  Tensor in that place has shape: (1024,)\n",
    "  Not inserted tensor of shape (2,)\n",
    "  Tensor in that place has shape: (1024,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4fa54",
   "metadata": {},
   "source": [
    "### Part4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "def test_model(preds, target):\n",
    "    \"\"\"Function to test the model.\n",
    "\n",
    "    Args:\n",
    "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
    "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
    "\n",
    "    Returns:\n",
    "        float: log_perplexity of the model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    total_log_ppx = np.sum(tl.one_hot(target, preds.shape[-1]) * preds, axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
    "\n",
    "    non_pad = 1.0 - np.equal(targets, 0)          # You should check if the target equals 0\n",
    "    ppx = non_pad * total_log_ppx                             # Get rid of the padding\n",
    "\n",
    "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# Testing \n",
    "model = GRULM()\n",
    "model.init_from_file('model.pkl.gz')\n",
    "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
    "preds = model(batch[0])\n",
    "log_ppx = test_model(preds, batch[1])\n",
    "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212c07b",
   "metadata": {},
   "source": [
    "### Generating the language with your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate some news sentence\n",
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "       \n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "    \n",
    "    return \"\".join(result)\n",
    "\n",
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e13d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
