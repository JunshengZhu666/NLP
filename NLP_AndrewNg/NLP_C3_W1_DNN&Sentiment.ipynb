{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa029970",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3afdcd9c7c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trax'"
     ]
    }
   ],
   "source": [
    "import trax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0b4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "686da378",
   "metadata": {},
   "source": [
    "# Lecture 1 on trax introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f94f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # regular ol' numpy\n",
    "\n",
    "from trax import layers as tl  # core building block\n",
    "from trax import shapes  # data signatures: dimensionality and type\n",
    "from trax import fastmath  # uses jax, offers numpy on steroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53e0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "-- Properties --\n",
      "name : Serial\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Layers\n",
    "# Create a relu trax layer\n",
    "relu = tl.Relu()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", relu.name)\n",
    "print(\"expected inputs :\", relu.n_in)\n",
    "print(\"promised outputs :\", relu.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = relu(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49faf551",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609d53a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Serial\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Layers\n",
    "# Create a relu trax layer\n",
    "relu = tl.Relu()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", relu.name)\n",
    "print(\"expected inputs :\", relu.n_in)\n",
    "print(\"promised outputs :\", relu.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = relu(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c0503",
   "metadata": {},
   "source": [
    "### Concatenate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520863ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10. -20. -30.   1.   2.   3.]\n"
     ]
    }
   ],
   "source": [
    "# Create a concatenate trax layer\n",
    "concat = tl.Concatenate()\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat.name)\n",
    "print(\"expected inputs :\", concat.n_in)\n",
    "print(\"promised outputs :\", concat.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat([x1, x2])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3c052",
   "metadata": {},
   "source": [
    "### Configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a35114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 3\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.]\n",
      "x3 : [0.99 1.98 2.97] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n"
     ]
    }
   ],
   "source": [
    "# Configure a concatenate layer\n",
    "concat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat_3.name)\n",
    "print(\"expected inputs :\", concat_3.n_in)\n",
    "print(\"promised outputs :\", concat_3.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "x3 = x2 * 0.99\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2)\n",
    "print(\"x3 :\", x3, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat_3([x1, x2, x3])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca73d5",
   "metadata": {},
   "source": [
    "### Layers can have weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c3533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal shape: (4,) Data Type: <class 'tuple'>\n",
      "Shapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: <class 'trax.shapes.ShapeDtype'>\n",
      "-- Properties --\n",
      "name : LayerNorm\n",
      "expected inputs : 1\n",
      "promised outputs : 1\n",
      "weights : [1. 1. 1. 1.]\n",
      "biases : [0. 0. 0. 0.] \n",
      "\n",
      "-- Inputs --\n",
      "x : [0. 1. 2. 3.]\n",
      "-- Outputs --\n",
      "y : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]\n",
      "/home/junsheng/anaconda3/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3094: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"ones\")\n",
      "/home/junsheng/anaconda3/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3085: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    }
   ],
   "source": [
    "# Layer initialization\n",
    "norm = tl.LayerNorm()\n",
    "# You first must know what the input data will look like\n",
    "x = np.array([0, 1, 2, 3], dtype=\"float\")\n",
    "\n",
    "# Use the input data signature to get shape and type for initializing weights and biases\n",
    "norm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n",
    "\n",
    "print(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\n",
    "print(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", norm.name)\n",
    "print(\"expected inputs :\", norm.n_in)\n",
    "print(\"promised outputs :\", norm.n_out)\n",
    "# Weights and biases\n",
    "print(\"weights :\", norm.weights[0])\n",
    "print(\"biases :\", norm.weights[1], \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x)\n",
    "\n",
    "# Outputs\n",
    "y = norm(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8454175",
   "metadata": {},
   "source": [
    "### Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62290464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : TimesTwo\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [1 2 3] \n",
      "\n",
      "-- Outputs --\n",
      "y : [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Define a custom layer\n",
    "# In this example you will create a layer to calculate the input times 2\n",
    "\n",
    "def TimesTwo():\n",
    "    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x):\n",
    "        return x * 2\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "\n",
    "# Test it\n",
    "times_two = TimesTwo()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", times_two.name)\n",
    "print(\"expected inputs :\", times_two.n_in)\n",
    "print(\"promised outputs :\", times_two.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = times_two(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46305618",
   "metadata": {},
   "source": [
    "### Serial Combinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1456f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial[\n",
      "  LayerNorm\n",
      "  Serial[\n",
      "    Relu\n",
      "  ]\n",
      "  TimesTwo\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "sublayers : [LayerNorm, Serial[\n",
      "  Relu\n",
      "], TimesTwo]\n",
      "expected inputs : 1\n",
      "promised outputs : 1\n",
      "weights & biases: ((DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), ((), (), ()), ()) \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0.        0.        0.        1.4142132 2.8284264]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junsheng/anaconda3/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3094: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"ones\")\n",
      "/home/junsheng/anaconda3/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3085: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    }
   ],
   "source": [
    "# Serial combinator\n",
    "serial = tl.Serial(\n",
    "    tl.LayerNorm(),         # normalize input\n",
    "    tl.Relu(),              # convert negative values to zero\n",
    "    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n",
    "    \n",
    "    ### START CODE HERE\n",
    "#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n",
    "#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n",
    "#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n",
    "    ### END CODE HERE\n",
    ")\n",
    "\n",
    "# Initialization\n",
    "x = np.array([-2, -1, 0, 1, 2]) #input\n",
    "serial.init(shapes.signature(x)) #initialising serial instance\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial,\"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out)\n",
    "print(\"weights & biases:\", serial.weights, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbee1fe",
   "metadata": {},
   "source": [
    "# Lecture 2 on Class and Subclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b3cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_a: None\n",
      "Parameter x of instance_b: None\n",
      "Parameter x of instance_a: 5\n"
     ]
    }
   ],
   "source": [
    "class My_Class: #Definition of My_class\n",
    "    x = None\n",
    "    \n",
    "instance_a= My_Class() #To create an instance from class \"My_Class\" you have to call \"My_Class\"\n",
    "instance_b= My_Class()\n",
    "print('Parameter x of instance_a: ' + str(instance_a.x)) #To get a parameter 'x' from an instance 'a', write 'a.x'\n",
    "print('Parameter x of instance_b: ' + str(instance_b.x))\n",
    "\n",
    "### START CODE HERE (1 line) ### \n",
    "instance_a.x = 5\n",
    "### END CODE HERE ###\n",
    "print('Parameter x of instance_a: ' + str(instance_a.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab693780",
   "metadata": {},
   "source": [
    "### The init method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5d8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n",
    "        self.x = y         # Sets parameter x to be equal to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce864e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_c: 10\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE (1 line) ### \n",
    "instance_c = My_Class(10)\n",
    "### END CODE HERE ###\n",
    "print('Parameter x of instance_c: ' + str(instance_c.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11ea14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n",
    "        self.x = y         # Sets parameter x to be equal to y\n",
    "    def __call__(self, z): # __call__ method with self and z as arguments\n",
    "        self.x += z        # Adds z to parameter x when called \n",
    "        print(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80040cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "instance_d = My_Class(5)\n",
    "instance_d(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc15252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        ### START CODE HERE (2 lines) ### \n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "        ### END CODE HERE ###\n",
    "    def __call__(self):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n",
    "        ### START CODE HERE (1 line) ### \n",
    "        result = self.x_1 + self.x_2 \n",
    "        ### END CODE HERE ### \n",
    "        print(\"Addition of {} and {} is {}\".format(self.x_1,self.x_2,result))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "562028d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of 10 and 15 is 25\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "instance_e = My_Class(10,15)\n",
    "def test_class_definition():\n",
    "    \n",
    "    assert instance_e.x_1 == 10, \"Check the value assigned to x_1\"\n",
    "    assert instance_e.x_2 == 15, \"Check the value assigned to x_2\"\n",
    "    assert instance_e() == 25, \"Check the __call__ method\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "test_class_definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5999d42f",
   "metadata": {},
   "source": [
    "### Custom method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0a19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):       #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):   #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        result = self.x_1*self.x_2 + w\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e18be8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of my_method: 26\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE (1 line) ### \n",
    "instance_f = My_Class(1,10)\n",
    "### END CODE HERE ### \n",
    "print(\"Output of my_method:\",instance_f.my_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8760a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z):         #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):               #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2 * self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):           #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        b = self.x_1 * self.x_2 + w\n",
    "        return b\n",
    "    def new_method(self, v):          #Calls My_method with argument v\n",
    "        ### START CODE HERE (1 line) ### \n",
    "        result = self.my_method(v)\n",
    "        ### END CODE HERE ### \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63e8cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden-cell\n",
    "class My_Class: \n",
    "    def __init__(self, y, z):      #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):            #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):        #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        b = self.x_1*self.x_2 + w\n",
    "        return b\n",
    "    def new_method(self, v):       #Calls My_method with argument v\n",
    "        result = self.my_method(v)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f353fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of my_method: 26\n",
      "Output of new_method: 26\n"
     ]
    }
   ],
   "source": [
    "instance_g = My_Class(1,10)\n",
    "print(\"Output of my_method:\",instance_g.my_method(16))\n",
    "print(\"Output of new_method:\",instance_g.new_method(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742d1e5",
   "metadata": {},
   "source": [
    "### Subclass and Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7fd5eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x_1 of instance_sub_a: 1\n",
      "Parameter x_2 of instance_sub_a: 10\n",
      "Output of my_method of instance_sub_a: 26\n"
     ]
    }
   ],
   "source": [
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def additional_method(self): #Prints the value of parameter x_1\n",
    "        print(self.x_1)\n",
    "        \n",
    "instance_sub_a = sub_c(1,10)\n",
    "print('Parameter x_1 of instance_sub_a: ' + str(instance_sub_a.x_1))\n",
    "print('Parameter x_2 of instance_sub_a: ' + str(instance_sub_a.x_2))\n",
    "print(\"Output of my_method of instance_sub_a:\",instance_sub_a.my_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1992604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def my_method(self):         #Multiplies x_1 and x_2 and returns the result\n",
    "        ### START CODE HERE (1 line) ###\n",
    "        b = self.x_1*self.x_2 \n",
    "        ### END CODE HERE ###\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "370346fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of overridden my_method of test: 30\n"
     ]
    }
   ],
   "source": [
    "test = sub_c(3,10)\n",
    "assert test.my_method() == 30, \"The method my_method should return the product between x_1 and x_2\"\n",
    "\n",
    "print(\"Output of overridden my_method of test:\",test.my_method()) #notice we didn't pass any parameter to call my_method\n",
    "#print(\"Output of overridden my_method of test:\",test.my_method(16)) #try to see what happens if you call it with 1 argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07bdd6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_method for an instance of sub_c returns: 10\n",
      "My_method for an instance of My_Class returns: 20\n"
     ]
    }
   ],
   "source": [
    "y,z= 1,10\n",
    "instance_sub_a = sub_c(y,z)\n",
    "instance_a = My_Class(y,z)\n",
    "print('My_method for an instance of sub_c returns: ' + str(instance_sub_a.my_method()))\n",
    "print('My_method for an instance of My_Class returns: ' + str(instance_a.my_method(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040bfa3",
   "metadata": {},
   "source": [
    "# Data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc531b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "\n",
    "# Example of traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = [0] * 10\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = len(b)\n",
    "lines_index = [*range(a_size)] # is equivalent to [i for i in range(0,a_size)], the difference being the advantage of using * to pass values of range iterator to list directly\n",
    "index = 0                      # similar to index in data_generator below\n",
    "for i in range(b_size):        # `b` is longer than `a` forcing a wrap\n",
    "    # We wrap by resetting index to 0 so the sequences circle back at the end to point to the first index\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "    \n",
    "    b[i] = a[lines_index[index]]     #  `indexes_list[index]` point to a index of a. Store the result in b\n",
    "    index += 1\n",
    "    \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f07c3881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original order of index: [0, 1, 2, 3]\n",
      "Shuffled order of index: [2, 0, 1, 3]\n",
      "New value order for first batch: [3, 1, 2, 4]\n",
      "\n",
      "Shuffled Indexes for Batch No.2 :[2, 3, 1, 0]\n",
      "Values for Batch No.2 :[3, 4, 2, 1]\n",
      "\n",
      "Shuffled Indexes for Batch No.3 :[2, 0, 3, 1]\n",
      "Values for Batch No.3 :[3, 1, 4, 2]\n",
      "\n",
      "Final value of b: [3, 1, 2, 4, 3, 4, 2, 1, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Example of traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = []\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = 10\n",
    "lines_index = [*range(a_size)]\n",
    "print(\"Original order of index:\",lines_index)\n",
    "\n",
    "# if we shuffle the index_list we can change the order of our circular list\n",
    "# without modifying the order or our original data\n",
    "random.shuffle(lines_index) # Shuffle the order\n",
    "print(\"Shuffled order of index:\",lines_index)\n",
    "\n",
    "print(\"New value order for first batch:\",[a[index] for index in lines_index])\n",
    "batch_counter = 1\n",
    "index = 0                # similar to index in data_generator below\n",
    "for i in range(b_size):  # `b` is longer than `a` forcing a wrap\n",
    "    # We wrap by resetting index to 0\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "        batch_counter += 1\n",
    "        random.shuffle(lines_index) # Re-shuffle the order\n",
    "        print(\"\\nShuffled Indexes for Batch No.{} :{}\".format(batch_counter,lines_index))\n",
    "        print(\"Values for Batch No.{} :{}\".format(batch_counter,[a[index] for index in lines_index]))\n",
    "    \n",
    "    b.append(a[lines_index[index]])     #  `indexes_list[index]` point to a index of a. Store the result in b\n",
    "    index += 1\n",
    "print()    \n",
    "print(\"Final value of b:\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, data_x, data_y, shuffle=True):\n",
    "    '''\n",
    "      Input: \n",
    "        batch_size - integer describing the batch size\n",
    "        data_x - list containing samples\n",
    "        data_y - list containing labels\n",
    "        shuffle - Shuffle the data order\n",
    "      Output:\n",
    "        a tuple containing 2 elements:\n",
    "        X - list of dim (batch_size) of samples\n",
    "        Y - list of dim (batch_size) of labels\n",
    "    '''\n",
    "    \n",
    "    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n",
    "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n",
    "    \n",
    "    \n",
    "    # If shuffle is set to true, we traverse the list in a random way\n",
    "    if shuffle:\n",
    "        random.shuffle(index_list) # Inplace shuffle of the list\n",
    "    \n",
    "    index = 0 # Start with the first element\n",
    "    # START CODE HERE    \n",
    "    # Fill all the None values with code taking reference of what you learned so far\n",
    "    while True:\n",
    "        X = None # We can create a list with batch_size elements. \n",
    "        Y = None # We can create a list with batch_size elements. \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Wrap the index each time that we reach the end of the list\n",
    "            if index >= data_lng:\n",
    "                index = None\n",
    "                # Shuffle the index_list if shuffle is true\n",
    "                if shuffle:\n",
    "                    None # re-shuffle the order\n",
    "            \n",
    "            X[i] = None # We set the corresponding element in x\n",
    "            Y[i] = None # We set the corresponding element in x\n",
    "    # END CODE HERE            \n",
    "            index += 1\n",
    "        \n",
    "        yield((X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a44a13",
   "metadata": {},
   "source": [
    "# Assignment1: Sentiment with Deep Neural Networks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "387a4f2d",
   "metadata": {},
   "source": [
    "Part 1: Import libraries and try out Trax\n",
    "Part 2: Importing the data\n",
    "    2.1 Loading in the data\n",
    "    2.2 Building the vocabulary\n",
    "    2.3 Converting a tweet to a tensor\n",
    "        Exercise 01\n",
    "    2.4 Creating a batch generator\n",
    "        Exercise 02\n",
    "Part 3: Defining classes\n",
    "    3.1 ReLU class\n",
    "        Exercise 03\n",
    "3.2 Dense class\n",
    "        Exercise 04\n",
    "3.3 Model\n",
    "        Exercise 05\n",
    "Part 4: Training\n",
    "    4.1 Training the model\n",
    "        Exercise 06\n",
    "    4.2 Practice Making a prediction\n",
    "Part 5: Evaluation\n",
    "    5.1 Computing the accuracy on a batch\n",
    "        Exercise 07\n",
    "    5.2 Testing your model on Validation Data\n",
    "        Exercise 08\n",
    "Part 6: Testing with your own input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092c3e2",
   "metadata": {},
   "source": [
    "### Part1: Trax and some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06a7562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading twitter_samples: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import random as rnd\n",
    "\n",
    "# import relevant libraries\n",
    "import trax\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "# trax.supervised.trainer_lib.init_random_number_generators(31)\n",
    "# In 1.3.4 we made training.Loop take the random_seed argument, \n",
    "# so you can train in a deterministic way with that. S\n",
    "# so you can just pass it to training.Loop(..., random_seed=31).\n",
    "\n",
    "# import trax.fastmath.numpy\n",
    "import trax.fastmath.numpy as np\n",
    "\n",
    "# import trax.layers\n",
    "from trax import layers as tl\n",
    "\n",
    "# import Layer from the utils.py file\n",
    "from utils import Layer, load_tweets, process_tweet\n",
    "#from utils import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302bd5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(5., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# create an array using trax.fastmath.numpy\n",
    "a = np.array(5.0)\n",
    "\n",
    "# view\n",
    "display(a)\n",
    "\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a77624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will use the trax.fastmath.numpy array\n",
    "def f(x):\n",
    "    \n",
    "    # f = x^2\n",
    "    return (x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b11673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0 is 25.0\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "print(f\"f(a) for a={a} is {f(a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d10d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to calcualte the gradient using fastmath\n",
    "grad_f = trax.fastmath.grad(fun=f)\n",
    "\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5393a0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(10., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the grad_f\n",
    "grad_calculation = grad_f(a)\n",
    "\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ba1d7",
   "metadata": {},
   "source": [
    "### Part2: Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5205fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 8000\n",
      "length of val_x 2000\n"
     ]
    }
   ],
   "source": [
    "## DO NOT EDIT THIS CELL\n",
    "\n",
    "# Import functions from the utils.py file\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load positive and negative tweets\n",
    "all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "# View the total number of positive and negative tweets.\n",
    "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "# Split positive set into validation and training\n",
    "val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "# Split negative set into validation and training\n",
    "val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "\n",
    "# Combine training data into one set\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# Combine validation data into one set\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "# Set the labels for the training set (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9273850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tweet at training position 0\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Tweet at training position 0 after processing:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a function that processes the tweets\n",
    "# from utils import process_tweet\n",
    "\n",
    "# Try out function that processes tweets\n",
    "print(\"original tweet at training position 0\")\n",
    "print(train_pos[0])\n",
    "\n",
    "print(\"Tweet at training position 0 after processing:\")\n",
    "process_tweet(train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4b038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e4c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'followfriday': 3,\n",
       " 'top': 4,\n",
       " 'engag': 5,\n",
       " 'member': 6,\n",
       " 'commun': 7,\n",
       " 'week': 8,\n",
       " ':)': 9,\n",
       " 'hey': 10,\n",
       " 'jame': 11,\n",
       " 'odd': 12,\n",
       " ':/': 13,\n",
       " 'pleas': 14,\n",
       " 'call': 15,\n",
       " 'contact': 16,\n",
       " 'centr': 17,\n",
       " '02392441234': 18,\n",
       " 'abl': 19,\n",
       " 'assist': 20,\n",
       " 'mani': 21,\n",
       " 'thank': 22,\n",
       " 'listen': 23,\n",
       " 'last': 24,\n",
       " 'night': 25,\n",
       " 'bleed': 26,\n",
       " 'amaz': 27,\n",
       " 'track': 28,\n",
       " 'scotland': 29,\n",
       " 'congrat': 30,\n",
       " 'yeaaah': 31,\n",
       " 'yipppi': 32,\n",
       " 'accnt': 33,\n",
       " 'verifi': 34,\n",
       " 'rqst': 35,\n",
       " 'succeed': 36,\n",
       " 'got': 37,\n",
       " 'blue': 38,\n",
       " 'tick': 39,\n",
       " 'mark': 40,\n",
       " 'fb': 41,\n",
       " 'profil': 42,\n",
       " '15': 43,\n",
       " 'day': 44,\n",
       " 'one': 45,\n",
       " 'irresist': 46,\n",
       " 'flipkartfashionfriday': 47,\n",
       " 'like': 48,\n",
       " 'keep': 49,\n",
       " 'love': 50,\n",
       " 'custom': 51,\n",
       " 'wait': 52,\n",
       " 'long': 53,\n",
       " 'hope': 54,\n",
       " 'enjoy': 55,\n",
       " 'happi': 56,\n",
       " 'friday': 57,\n",
       " 'lwwf': 58,\n",
       " 'second': 59,\n",
       " 'thought': 60,\n",
       " '’': 61,\n",
       " 'enough': 62,\n",
       " 'time': 63,\n",
       " 'dd': 64,\n",
       " 'new': 65,\n",
       " 'short': 66,\n",
       " 'enter': 67,\n",
       " 'system': 68,\n",
       " 'sheep': 69,\n",
       " 'must': 70,\n",
       " 'buy': 71,\n",
       " 'jgh': 72,\n",
       " 'go': 73,\n",
       " 'bayan': 74,\n",
       " ':d': 75,\n",
       " 'bye': 76,\n",
       " 'act': 77,\n",
       " 'mischiev': 78,\n",
       " 'etl': 79,\n",
       " 'layer': 80,\n",
       " 'in-hous': 81,\n",
       " 'wareh': 82,\n",
       " 'app': 83,\n",
       " 'katamari': 84,\n",
       " 'well': 85,\n",
       " '…': 86,\n",
       " 'name': 87,\n",
       " 'impli': 88,\n",
       " ':p': 89,\n",
       " 'influenc': 90,\n",
       " 'big': 91,\n",
       " '...': 92,\n",
       " 'juici': 93,\n",
       " 'selfi': 94,\n",
       " 'follow': 95,\n",
       " 'perfect': 96,\n",
       " 'alreadi': 97,\n",
       " 'know': 98,\n",
       " \"what'\": 99,\n",
       " 'great': 100,\n",
       " 'opportun': 101,\n",
       " 'junior': 102,\n",
       " 'triathlet': 103,\n",
       " 'age': 104,\n",
       " '12': 105,\n",
       " '13': 106,\n",
       " 'gatorad': 107,\n",
       " 'seri': 108,\n",
       " 'get': 109,\n",
       " 'entri': 110,\n",
       " 'lay': 111,\n",
       " 'greet': 112,\n",
       " 'card': 113,\n",
       " 'rang': 114,\n",
       " 'print': 115,\n",
       " 'today': 116,\n",
       " 'job': 117,\n",
       " ':-)': 118,\n",
       " \"friend'\": 119,\n",
       " 'lunch': 120,\n",
       " 'yummm': 121,\n",
       " 'nostalgia': 122,\n",
       " 'tb': 123,\n",
       " 'ku': 124,\n",
       " 'id': 125,\n",
       " 'conflict': 126,\n",
       " 'help': 127,\n",
       " \"here'\": 128,\n",
       " 'screenshot': 129,\n",
       " 'work': 130,\n",
       " 'hi': 131,\n",
       " 'liv': 132,\n",
       " 'hello': 133,\n",
       " 'need': 134,\n",
       " 'someth': 135,\n",
       " 'u': 136,\n",
       " 'fm': 137,\n",
       " 'twitter': 138,\n",
       " '—': 139,\n",
       " 'sure': 140,\n",
       " 'thing': 141,\n",
       " 'dm': 142,\n",
       " 'x': 143,\n",
       " \"i'v\": 144,\n",
       " 'heard': 145,\n",
       " 'four': 146,\n",
       " 'season': 147,\n",
       " 'pretti': 148,\n",
       " 'dope': 149,\n",
       " 'penthous': 150,\n",
       " 'obv': 151,\n",
       " 'gobigorgohom': 152,\n",
       " 'fun': 153,\n",
       " \"y'all\": 154,\n",
       " 'yeah': 155,\n",
       " 'suppos': 156,\n",
       " 'lol': 157,\n",
       " 'chat': 158,\n",
       " 'bit': 159,\n",
       " 'youth': 160,\n",
       " '💅': 161,\n",
       " '🏽': 162,\n",
       " '💋': 163,\n",
       " 'seen': 164,\n",
       " 'year': 165,\n",
       " 'rest': 166,\n",
       " 'goe': 167,\n",
       " 'quickli': 168,\n",
       " 'bed': 169,\n",
       " 'music': 170,\n",
       " 'fix': 171,\n",
       " 'dream': 172,\n",
       " 'spiritu': 173,\n",
       " 'ritual': 174,\n",
       " 'festiv': 175,\n",
       " 'népal': 176,\n",
       " 'begin': 177,\n",
       " 'line-up': 178,\n",
       " 'left': 179,\n",
       " 'see': 180,\n",
       " 'sarah': 181,\n",
       " 'send': 182,\n",
       " 'us': 183,\n",
       " 'email': 184,\n",
       " 'bitsy@bitdefender.com': 185,\n",
       " \"we'll\": 186,\n",
       " 'asap': 187,\n",
       " 'kik': 188,\n",
       " 'hatessuc': 189,\n",
       " '32429': 190,\n",
       " 'kikm': 191,\n",
       " 'lgbt': 192,\n",
       " 'tinder': 193,\n",
       " 'nsfw': 194,\n",
       " 'akua': 195,\n",
       " 'cumshot': 196,\n",
       " 'come': 197,\n",
       " 'hous': 198,\n",
       " 'nsn_supplement': 199,\n",
       " 'effect': 200,\n",
       " 'press': 201,\n",
       " 'releas': 202,\n",
       " 'distribut': 203,\n",
       " 'result': 204,\n",
       " 'link': 205,\n",
       " 'remov': 206,\n",
       " 'pressreleas': 207,\n",
       " 'newsdistribut': 208,\n",
       " 'bam': 209,\n",
       " 'bestfriend': 210,\n",
       " 'lot': 211,\n",
       " 'warsaw': 212,\n",
       " '<3': 213,\n",
       " 'x46': 214,\n",
       " 'everyon': 215,\n",
       " 'watch': 216,\n",
       " 'documentari': 217,\n",
       " 'earthl': 218,\n",
       " 'youtub': 219,\n",
       " 'support': 220,\n",
       " 'buuut': 221,\n",
       " 'oh': 222,\n",
       " 'look': 223,\n",
       " 'forward': 224,\n",
       " 'visit': 225,\n",
       " 'next': 226,\n",
       " 'letsgetmessi': 227,\n",
       " 'jo': 228,\n",
       " 'make': 229,\n",
       " 'feel': 230,\n",
       " 'better': 231,\n",
       " 'never': 232,\n",
       " 'anyon': 233,\n",
       " 'kpop': 234,\n",
       " 'flesh': 235,\n",
       " 'good': 236,\n",
       " 'girl': 237,\n",
       " 'best': 238,\n",
       " 'wish': 239,\n",
       " 'reason': 240,\n",
       " 'epic': 241,\n",
       " 'soundtrack': 242,\n",
       " 'shout': 243,\n",
       " 'ad': 244,\n",
       " 'video': 245,\n",
       " 'playlist': 246,\n",
       " 'would': 247,\n",
       " 'dear': 248,\n",
       " 'jordan': 249,\n",
       " 'okay': 250,\n",
       " 'fake': 251,\n",
       " 'gameplay': 252,\n",
       " ';)': 253,\n",
       " 'haha': 254,\n",
       " 'im': 255,\n",
       " 'kid': 256,\n",
       " 'stuff': 257,\n",
       " 'exactli': 258,\n",
       " 'product': 259,\n",
       " 'line': 260,\n",
       " 'etsi': 261,\n",
       " 'shop': 262,\n",
       " 'check': 263,\n",
       " 'vacat': 264,\n",
       " 'recharg': 265,\n",
       " 'normal': 266,\n",
       " 'charger': 267,\n",
       " 'asleep': 268,\n",
       " 'talk': 269,\n",
       " 'sooo': 270,\n",
       " 'someon': 271,\n",
       " 'text': 272,\n",
       " 'ye': 273,\n",
       " 'bet': 274,\n",
       " \"he'll\": 275,\n",
       " 'fit': 276,\n",
       " 'hear': 277,\n",
       " 'speech': 278,\n",
       " 'piti': 279,\n",
       " 'green': 280,\n",
       " 'garden': 281,\n",
       " 'midnight': 282,\n",
       " 'sun': 283,\n",
       " 'beauti': 284,\n",
       " 'canal': 285,\n",
       " 'dasvidaniya': 286,\n",
       " 'till': 287,\n",
       " 'scout': 288,\n",
       " 'sg': 289,\n",
       " 'futur': 290,\n",
       " 'wlan': 291,\n",
       " 'pro': 292,\n",
       " 'confer': 293,\n",
       " 'asia': 294,\n",
       " 'chang': 295,\n",
       " 'lollipop': 296,\n",
       " '🍭': 297,\n",
       " 'nez': 298,\n",
       " 'agnezmo': 299,\n",
       " 'oley': 300,\n",
       " 'mama': 301,\n",
       " 'stand': 302,\n",
       " 'stronger': 303,\n",
       " 'god': 304,\n",
       " 'misti': 305,\n",
       " 'babi': 306,\n",
       " 'cute': 307,\n",
       " 'woohoo': 308,\n",
       " \"can't\": 309,\n",
       " 'sign': 310,\n",
       " 'yet': 311,\n",
       " 'still': 312,\n",
       " 'think': 313,\n",
       " 'mka': 314,\n",
       " 'liam': 315,\n",
       " 'access': 316,\n",
       " 'welcom': 317,\n",
       " 'stat': 318,\n",
       " 'arriv': 319,\n",
       " '1': 320,\n",
       " 'unfollow': 321,\n",
       " 'via': 322,\n",
       " 'surpris': 323,\n",
       " 'figur': 324,\n",
       " 'happybirthdayemilybett': 325,\n",
       " 'sweet': 326,\n",
       " 'talent': 327,\n",
       " '2': 328,\n",
       " 'plan': 329,\n",
       " 'drain': 330,\n",
       " 'gotta': 331,\n",
       " 'timezon': 332,\n",
       " 'parent': 333,\n",
       " 'proud': 334,\n",
       " 'least': 335,\n",
       " 'mayb': 336,\n",
       " 'sometim': 337,\n",
       " 'grade': 338,\n",
       " 'al': 339,\n",
       " 'grand': 340,\n",
       " 'manila_bro': 341,\n",
       " 'chosen': 342,\n",
       " 'let': 343,\n",
       " 'around': 344,\n",
       " '..': 345,\n",
       " 'side': 346,\n",
       " 'world': 347,\n",
       " 'eh': 348,\n",
       " 'take': 349,\n",
       " 'care': 350,\n",
       " 'final': 351,\n",
       " 'fuck': 352,\n",
       " 'weekend': 353,\n",
       " 'real': 354,\n",
       " 'x45': 355,\n",
       " 'join': 356,\n",
       " 'hushedcallwithfraydo': 357,\n",
       " 'gift': 358,\n",
       " 'yeahhh': 359,\n",
       " 'hushedpinwithsammi': 360,\n",
       " 'event': 361,\n",
       " 'might': 362,\n",
       " 'luv': 363,\n",
       " 'realli': 364,\n",
       " 'appreci': 365,\n",
       " 'share': 366,\n",
       " 'wow': 367,\n",
       " 'tom': 368,\n",
       " 'gym': 369,\n",
       " 'monday': 370,\n",
       " 'invit': 371,\n",
       " 'scope': 372,\n",
       " 'friend': 373,\n",
       " 'nude': 374,\n",
       " 'sleep': 375,\n",
       " 'birthday': 376,\n",
       " 'want': 377,\n",
       " 't-shirt': 378,\n",
       " 'cool': 379,\n",
       " 'haw': 380,\n",
       " 'phela': 381,\n",
       " 'mom': 382,\n",
       " 'obvious': 383,\n",
       " 'princ': 384,\n",
       " 'charm': 385,\n",
       " 'stage': 386,\n",
       " 'luck': 387,\n",
       " 'tyler': 388,\n",
       " 'hipster': 389,\n",
       " 'glass': 390,\n",
       " 'marti': 391,\n",
       " 'glad': 392,\n",
       " 'done': 393,\n",
       " 'afternoon': 394,\n",
       " 'read': 395,\n",
       " 'kahfi': 396,\n",
       " 'finish': 397,\n",
       " 'ohmyg': 398,\n",
       " 'yaya': 399,\n",
       " 'dub': 400,\n",
       " 'stalk': 401,\n",
       " 'ig': 402,\n",
       " 'gondooo': 403,\n",
       " 'moo': 404,\n",
       " 'tologooo': 405,\n",
       " 'becom': 406,\n",
       " 'detail': 407,\n",
       " 'zzz': 408,\n",
       " 'xx': 409,\n",
       " 'physiotherapi': 410,\n",
       " 'hashtag': 411,\n",
       " '💪': 412,\n",
       " 'monica': 413,\n",
       " 'miss': 414,\n",
       " 'sound': 415,\n",
       " 'morn': 416,\n",
       " \"that'\": 417,\n",
       " 'x43': 418,\n",
       " 'definit': 419,\n",
       " 'tri': 420,\n",
       " 'tonight': 421,\n",
       " 'took': 422,\n",
       " 'advic': 423,\n",
       " 'treviso': 424,\n",
       " 'concert': 425,\n",
       " 'citi': 426,\n",
       " 'countri': 427,\n",
       " \"i'll\": 428,\n",
       " 'start': 429,\n",
       " 'fine': 430,\n",
       " 'gorgeou': 431,\n",
       " 'xo': 432,\n",
       " 'oven': 433,\n",
       " 'roast': 434,\n",
       " 'garlic': 435,\n",
       " 'oliv': 436,\n",
       " 'oil': 437,\n",
       " 'dri': 438,\n",
       " 'tomato': 439,\n",
       " 'basil': 440,\n",
       " 'centuri': 441,\n",
       " 'tuna': 442,\n",
       " 'right': 443,\n",
       " 'back': 444,\n",
       " 'atchya': 445,\n",
       " 'even': 446,\n",
       " 'almost': 447,\n",
       " 'chanc': 448,\n",
       " 'cheer': 449,\n",
       " 'po': 450,\n",
       " 'ice': 451,\n",
       " 'cream': 452,\n",
       " 'agre': 453,\n",
       " '100': 454,\n",
       " 'heheheh': 455,\n",
       " 'that': 456,\n",
       " 'point': 457,\n",
       " 'stay': 458,\n",
       " 'home': 459,\n",
       " 'soon': 460,\n",
       " 'promis': 461,\n",
       " 'web': 462,\n",
       " 'whatsapp': 463,\n",
       " 'volta': 464,\n",
       " 'funcionar': 465,\n",
       " 'com': 466,\n",
       " 'iphon': 467,\n",
       " 'jailbroken': 468,\n",
       " 'later': 469,\n",
       " '34': 470,\n",
       " 'min': 471,\n",
       " 'leia': 472,\n",
       " 'appear': 473,\n",
       " 'hologram': 474,\n",
       " 'r2d2': 475,\n",
       " 'w': 476,\n",
       " 'messag': 477,\n",
       " 'obi': 478,\n",
       " 'wan': 479,\n",
       " 'sit': 480,\n",
       " 'luke': 481,\n",
       " 'inter': 482,\n",
       " '3': 483,\n",
       " 'ucl': 484,\n",
       " 'arsen': 485,\n",
       " 'small': 486,\n",
       " 'team': 487,\n",
       " 'pass': 488,\n",
       " '🚂': 489,\n",
       " 'dewsburi': 490,\n",
       " 'railway': 491,\n",
       " 'station': 492,\n",
       " 'dew': 493,\n",
       " 'west': 494,\n",
       " 'yorkshir': 495,\n",
       " '430': 496,\n",
       " 'smh': 497,\n",
       " '9:25': 498,\n",
       " 'live': 499,\n",
       " 'strang': 500,\n",
       " 'imagin': 501,\n",
       " 'megan': 502,\n",
       " 'masaantoday': 503,\n",
       " 'a4': 504,\n",
       " 'shweta': 505,\n",
       " 'tripathi': 506,\n",
       " '5': 507,\n",
       " '20': 508,\n",
       " 'kurta': 509,\n",
       " 'half': 510,\n",
       " 'number': 511,\n",
       " 'wsalelov': 512,\n",
       " 'ah': 513,\n",
       " 'larri': 514,\n",
       " 'anyway': 515,\n",
       " 'kinda': 516,\n",
       " 'goood': 517,\n",
       " 'life': 518,\n",
       " 'enn': 519,\n",
       " 'could': 520,\n",
       " 'warmup': 521,\n",
       " '15th': 522,\n",
       " 'bath': 523,\n",
       " 'dum': 524,\n",
       " 'andar': 525,\n",
       " 'ram': 526,\n",
       " 'sampath': 527,\n",
       " 'sona': 528,\n",
       " 'mohapatra': 529,\n",
       " 'samantha': 530,\n",
       " 'edward': 531,\n",
       " 'mein': 532,\n",
       " 'tulan': 533,\n",
       " 'razi': 534,\n",
       " 'wah': 535,\n",
       " 'josh': 536,\n",
       " 'alway': 537,\n",
       " 'smile': 538,\n",
       " 'pictur': 539,\n",
       " '16.20': 540,\n",
       " 'giveitup': 541,\n",
       " 'given': 542,\n",
       " 'ga': 543,\n",
       " 'subsidi': 544,\n",
       " 'initi': 545,\n",
       " 'propos': 546,\n",
       " 'delight': 547,\n",
       " 'yesterday': 548,\n",
       " 'x42': 549,\n",
       " 'lmaoo': 550,\n",
       " 'song': 551,\n",
       " 'ever': 552,\n",
       " 'shall': 553,\n",
       " 'littl': 554,\n",
       " 'throwback': 555,\n",
       " 'outli': 556,\n",
       " 'island': 557,\n",
       " 'cheung': 558,\n",
       " 'chau': 559,\n",
       " 'mui': 560,\n",
       " 'wo': 561,\n",
       " 'total': 562,\n",
       " 'differ': 563,\n",
       " 'kfckitchentour': 564,\n",
       " 'kitchen': 565,\n",
       " 'clean': 566,\n",
       " \"i'm\": 567,\n",
       " 'cusp': 568,\n",
       " 'test': 569,\n",
       " 'water': 570,\n",
       " 'reward': 571,\n",
       " 'arummzz': 572,\n",
       " \"let'\": 573,\n",
       " 'drive': 574,\n",
       " 'travel': 575,\n",
       " 'yogyakarta': 576,\n",
       " 'jeep': 577,\n",
       " 'indonesia': 578,\n",
       " 'instamood': 579,\n",
       " 'wanna': 580,\n",
       " 'skype': 581,\n",
       " 'may': 582,\n",
       " 'nice': 583,\n",
       " 'friendli': 584,\n",
       " 'pretend': 585,\n",
       " 'film': 586,\n",
       " 'congratul': 587,\n",
       " 'winner': 588,\n",
       " 'cheesydelight': 589,\n",
       " 'contest': 590,\n",
       " 'address': 591,\n",
       " 'guy': 592,\n",
       " 'market': 593,\n",
       " '24/7': 594,\n",
       " '14': 595,\n",
       " 'hour': 596,\n",
       " 'leav': 597,\n",
       " 'without': 598,\n",
       " 'delay': 599,\n",
       " 'actual': 600,\n",
       " 'easi': 601,\n",
       " 'guess': 602,\n",
       " 'train': 603,\n",
       " 'wd': 604,\n",
       " 'shift': 605,\n",
       " 'engin': 606,\n",
       " 'etc': 607,\n",
       " 'sunburn': 608,\n",
       " 'peel': 609,\n",
       " 'blog': 610,\n",
       " 'huge': 611,\n",
       " 'warm': 612,\n",
       " '☆': 613,\n",
       " 'complet': 614,\n",
       " 'triangl': 615,\n",
       " 'northern': 616,\n",
       " 'ireland': 617,\n",
       " 'sight': 618,\n",
       " 'smthng': 619,\n",
       " 'fr': 620,\n",
       " 'hug': 621,\n",
       " 'xoxo': 622,\n",
       " 'uu': 623,\n",
       " 'jaann': 624,\n",
       " 'topnewfollow': 625,\n",
       " 'connect': 626,\n",
       " 'wonder': 627,\n",
       " 'made': 628,\n",
       " 'fluffi': 629,\n",
       " 'insid': 630,\n",
       " 'pirouett': 631,\n",
       " 'moos': 632,\n",
       " 'trip': 633,\n",
       " 'philli': 634,\n",
       " 'decemb': 635,\n",
       " \"i'd\": 636,\n",
       " 'dude': 637,\n",
       " 'x41': 638,\n",
       " 'question': 639,\n",
       " 'flaw': 640,\n",
       " 'pain': 641,\n",
       " 'negat': 642,\n",
       " 'strength': 643,\n",
       " 'went': 644,\n",
       " 'solo': 645,\n",
       " 'move': 646,\n",
       " 'fav': 647,\n",
       " 'nirvana': 648,\n",
       " 'smell': 649,\n",
       " 'teen': 650,\n",
       " 'spirit': 651,\n",
       " 'rip': 652,\n",
       " 'ami': 653,\n",
       " 'winehous': 654,\n",
       " 'coupl': 655,\n",
       " 'tomhiddleston': 656,\n",
       " 'elizabetholsen': 657,\n",
       " 'yaytheylookgreat': 658,\n",
       " 'goodnight': 659,\n",
       " 'vid': 660,\n",
       " 'wake': 661,\n",
       " 'gonna': 662,\n",
       " 'shoot': 663,\n",
       " 'itti': 664,\n",
       " 'bitti': 665,\n",
       " 'teeni': 666,\n",
       " 'bikini': 667,\n",
       " 'much': 668,\n",
       " '4th': 669,\n",
       " 'togeth': 670,\n",
       " 'end': 671,\n",
       " 'xfile': 672,\n",
       " 'content': 673,\n",
       " 'rain': 674,\n",
       " 'fabul': 675,\n",
       " 'fantast': 676,\n",
       " '♡': 677,\n",
       " 'jb': 678,\n",
       " 'forev': 679,\n",
       " 'belieb': 680,\n",
       " 'nighti': 681,\n",
       " 'bug': 682,\n",
       " 'bite': 683,\n",
       " 'bracelet': 684,\n",
       " 'idea': 685,\n",
       " 'foundri': 686,\n",
       " 'game': 687,\n",
       " 'sens': 688,\n",
       " 'pic': 689,\n",
       " 'ef': 690,\n",
       " 'phone': 691,\n",
       " 'woot': 692,\n",
       " 'derek': 693,\n",
       " 'use': 694,\n",
       " 'parkshar': 695,\n",
       " 'gloucestershir': 696,\n",
       " 'aaaahhh': 697,\n",
       " 'man': 698,\n",
       " 'traffic': 699,\n",
       " 'stress': 700,\n",
       " 'reliev': 701,\n",
       " \"how'r\": 702,\n",
       " 'arbeloa': 703,\n",
       " 'turn': 704,\n",
       " '17': 705,\n",
       " 'omg': 706,\n",
       " 'say': 707,\n",
       " 'europ': 708,\n",
       " 'rise': 709,\n",
       " 'find': 710,\n",
       " 'hard': 711,\n",
       " 'believ': 712,\n",
       " 'uncount': 713,\n",
       " 'coz': 714,\n",
       " 'unlimit': 715,\n",
       " 'cours': 716,\n",
       " 'teamposit': 717,\n",
       " 'aldub': 718,\n",
       " '☕': 719,\n",
       " 'rita': 720,\n",
       " 'info': 721,\n",
       " \"we'd\": 722,\n",
       " 'way': 723,\n",
       " 'boy': 724,\n",
       " 'x40': 725,\n",
       " 'true': 726,\n",
       " 'sethi': 727,\n",
       " 'high': 728,\n",
       " 'exe': 729,\n",
       " 'skeem': 730,\n",
       " 'saam': 731,\n",
       " 'peopl': 732,\n",
       " 'polit': 733,\n",
       " 'izzat': 734,\n",
       " 'wese': 735,\n",
       " 'trust': 736,\n",
       " 'khawateen': 737,\n",
       " 'k': 738,\n",
       " 'sath': 739,\n",
       " 'mana': 740,\n",
       " 'kar': 741,\n",
       " 'deya': 742,\n",
       " 'sort': 743,\n",
       " 'smart': 744,\n",
       " 'hair': 745,\n",
       " 'tbh': 746,\n",
       " 'jacob': 747,\n",
       " 'g': 748,\n",
       " 'upgrad': 749,\n",
       " 'tee': 750,\n",
       " 'famili': 751,\n",
       " 'person': 752,\n",
       " 'two': 753,\n",
       " 'convers': 754,\n",
       " 'onlin': 755,\n",
       " 'mclaren': 756,\n",
       " 'fridayfeel': 757,\n",
       " 'tgif': 758,\n",
       " 'squar': 759,\n",
       " 'enix': 760,\n",
       " 'bissmillah': 761,\n",
       " 'ya': 762,\n",
       " 'allah': 763,\n",
       " \"we'r\": 764,\n",
       " 'socent': 765,\n",
       " 'startup': 766,\n",
       " 'drop': 767,\n",
       " 'your': 768,\n",
       " 'arnd': 769,\n",
       " 'town': 770,\n",
       " 'basic': 771,\n",
       " 'piss': 772,\n",
       " 'cup': 773,\n",
       " 'also': 774,\n",
       " 'terribl': 775,\n",
       " 'complic': 776,\n",
       " 'discuss': 777,\n",
       " 'snapchat': 778,\n",
       " 'lynettelow': 779,\n",
       " 'kikmenow': 780,\n",
       " 'snapm': 781,\n",
       " 'hot': 782,\n",
       " 'amazon': 783,\n",
       " 'kikmeguy': 784,\n",
       " 'defin': 785,\n",
       " 'grow': 786,\n",
       " 'sport': 787,\n",
       " 'rt': 788,\n",
       " 'rakyat': 789,\n",
       " 'write': 790,\n",
       " 'sinc': 791,\n",
       " 'mention': 792,\n",
       " 'fli': 793,\n",
       " 'fish': 794,\n",
       " 'promot': 795,\n",
       " 'post': 796,\n",
       " 'cyber': 797,\n",
       " 'ourdaughtersourprid': 798,\n",
       " 'mypapamyprid': 799,\n",
       " 'papa': 800,\n",
       " 'coach': 801,\n",
       " 'posit': 802,\n",
       " 'kha': 803,\n",
       " 'atleast': 804,\n",
       " 'x39': 805,\n",
       " 'mango': 806,\n",
       " \"lassi'\": 807,\n",
       " \"monty'\": 808,\n",
       " 'marvel': 809,\n",
       " 'though': 810,\n",
       " 'suspect': 811,\n",
       " 'meant': 812,\n",
       " '24': 813,\n",
       " 'hr': 814,\n",
       " 'touch': 815,\n",
       " 'kepler': 816,\n",
       " '452b': 817,\n",
       " 'chalna': 818,\n",
       " 'hai': 819,\n",
       " 'thankyou': 820,\n",
       " 'hazel': 821,\n",
       " 'food': 822,\n",
       " 'brooklyn': 823,\n",
       " 'pta': 824,\n",
       " 'awak': 825,\n",
       " 'okayi': 826,\n",
       " 'awww': 827,\n",
       " 'ha': 828,\n",
       " 'doc': 829,\n",
       " 'splendid': 830,\n",
       " 'spam': 831,\n",
       " 'folder': 832,\n",
       " 'amount': 833,\n",
       " 'nigeria': 834,\n",
       " 'claim': 835,\n",
       " 'rted': 836,\n",
       " 'leg': 837,\n",
       " 'hurt': 838,\n",
       " 'bad': 839,\n",
       " 'mine': 840,\n",
       " 'saturday': 841,\n",
       " 'thaaank': 842,\n",
       " 'puhon': 843,\n",
       " 'happinesss': 844,\n",
       " 'tnc': 845,\n",
       " 'prior': 846,\n",
       " 'notif': 847,\n",
       " 'fat': 848,\n",
       " 'co': 849,\n",
       " 'probabl': 850,\n",
       " 'ate': 851,\n",
       " 'yuna': 852,\n",
       " 'tamesid': 853,\n",
       " '´': 854,\n",
       " 'googl': 855,\n",
       " 'account': 856,\n",
       " 'scouser': 857,\n",
       " 'everyth': 858,\n",
       " 'zoe': 859,\n",
       " 'mate': 860,\n",
       " 'liter': 861,\n",
       " \"they'r\": 862,\n",
       " 'samee': 863,\n",
       " 'edgar': 864,\n",
       " 'updat': 865,\n",
       " 'log': 866,\n",
       " 'bring': 867,\n",
       " 'abe': 868,\n",
       " 'meet': 869,\n",
       " 'x38': 870,\n",
       " 'sigh': 871,\n",
       " 'dreamili': 872,\n",
       " 'pout': 873,\n",
       " 'eye': 874,\n",
       " 'quacketyquack': 875,\n",
       " 'funni': 876,\n",
       " 'happen': 877,\n",
       " 'phil': 878,\n",
       " 'em': 879,\n",
       " 'del': 880,\n",
       " 'rodder': 881,\n",
       " 'els': 882,\n",
       " 'play': 883,\n",
       " 'newest': 884,\n",
       " 'gamejam': 885,\n",
       " 'irish': 886,\n",
       " 'literatur': 887,\n",
       " 'inaccess': 888,\n",
       " \"kareena'\": 889,\n",
       " 'fan': 890,\n",
       " 'brain': 891,\n",
       " 'dot': 892,\n",
       " 'braindot': 893,\n",
       " 'fair': 894,\n",
       " 'rush': 895,\n",
       " 'either': 896,\n",
       " 'brandi': 897,\n",
       " '18': 898,\n",
       " 'carniv': 899,\n",
       " 'men': 900,\n",
       " 'put': 901,\n",
       " 'mask': 902,\n",
       " 'xavier': 903,\n",
       " 'forneret': 904,\n",
       " 'jennif': 905,\n",
       " 'site': 906,\n",
       " 'free': 907,\n",
       " '50.000': 908,\n",
       " '8': 909,\n",
       " 'ball': 910,\n",
       " 'pool': 911,\n",
       " 'coin': 912,\n",
       " 'edit': 913,\n",
       " 'trish': 914,\n",
       " '♥': 915,\n",
       " 'grate': 916,\n",
       " 'three': 917,\n",
       " 'comment': 918,\n",
       " 'wakeup': 919,\n",
       " 'besid': 920,\n",
       " 'dirti': 921,\n",
       " 'sex': 922,\n",
       " 'lmaooo': 923,\n",
       " '😤': 924,\n",
       " 'loui': 925,\n",
       " \"he'\": 926,\n",
       " 'throw': 927,\n",
       " 'caus': 928,\n",
       " 'inspir': 929,\n",
       " 'ff': 930,\n",
       " 'twoof': 931,\n",
       " 'gr8': 932,\n",
       " 'wkend': 933,\n",
       " 'kind': 934,\n",
       " 'exhaust': 935,\n",
       " 'word': 936,\n",
       " 'cheltenham': 937,\n",
       " 'area': 938,\n",
       " 'kale': 939,\n",
       " 'crisp': 940,\n",
       " 'ruin': 941,\n",
       " 'x37': 942,\n",
       " 'open': 943,\n",
       " 'worldwid': 944,\n",
       " 'outta': 945,\n",
       " 'sfvbeta': 946,\n",
       " 'vantast': 947,\n",
       " 'xcylin': 948,\n",
       " 'bundl': 949,\n",
       " 'show': 950,\n",
       " 'internet': 951,\n",
       " 'price': 952,\n",
       " 'realisticli': 953,\n",
       " 'pay': 954,\n",
       " 'net': 955,\n",
       " 'educ': 956,\n",
       " 'power': 957,\n",
       " 'weapon': 958,\n",
       " 'nelson': 959,\n",
       " 'mandela': 960,\n",
       " 'recent': 961,\n",
       " 'j': 962,\n",
       " 'chenab': 963,\n",
       " 'flow': 964,\n",
       " 'pakistan': 965,\n",
       " 'incredibleindia': 966,\n",
       " 'teenchoic': 967,\n",
       " 'choiceinternationalartist': 968,\n",
       " 'superjunior': 969,\n",
       " 'caught': 970,\n",
       " 'first': 971,\n",
       " 'salmon': 972,\n",
       " 'super-blend': 973,\n",
       " 'project': 974,\n",
       " 'youth@bipolaruk.org.uk': 975,\n",
       " 'awesom': 976,\n",
       " 'stream': 977,\n",
       " 'alma': 978,\n",
       " 'mater': 979,\n",
       " 'highschoolday': 980,\n",
       " 'clientvisit': 981,\n",
       " 'faith': 982,\n",
       " 'christian': 983,\n",
       " 'school': 984,\n",
       " 'lizaminnelli': 985,\n",
       " 'upcom': 986,\n",
       " 'uk': 987,\n",
       " '😄': 988,\n",
       " 'singl': 989,\n",
       " 'hill': 990,\n",
       " 'everi': 991,\n",
       " 'beat': 992,\n",
       " 'wrong': 993,\n",
       " 'readi': 994,\n",
       " 'natur': 995,\n",
       " 'pefumeri': 996,\n",
       " 'workshop': 997,\n",
       " 'neal': 998,\n",
       " 'yard': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "# Unit Test Note - There is no test set here only train/val\n",
    "\n",
    "# Include special tokens \n",
    "# started with pad, end of line and unk tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "display(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec385f62",
   "metadata": {},
   "source": [
    "### converting a twee to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620f65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: tweet_to_tensor\n",
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''  \n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Process the tweet into a list of words\n",
    "    # where only important words are kept (stop words removed)\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = []\n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID) \n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92f9fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1065, 136, 479, 2351, 745, 8147, 1123, 745, 53, 2, 2671, 791, 2, 2, 349, 601, 2, 3488, 1017, 597, 4558, 9, 1065, 157, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c5abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# test tweet_to_tensor\n",
    "\n",
    "def test_tweet_to_tensor():\n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": [val_pos[1], Vocab],\n",
    "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
    "            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"datatype_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":type([]),\n",
    "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"without_unk_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":6,\n",
    "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
    "        }\n",
    "    ]\n",
    "    count = 0\n",
    "    for test_case in test_cases:\n",
    "        \n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"datatype_check\":\n",
    "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"without_unk_check\":\n",
    "                assert None not in tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "    if count == 3:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(count,\" Tests passed out of 3\")\n",
    "test_tweet_to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17def89e",
   "metadata": {},
   "source": [
    "### creating a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec424480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED: Data generator\n",
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "        data_pos - Set of posstive examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch. Must be even\n",
    "        loop - True or False\n",
    "        vocab_dict - The words dictionary\n",
    "        shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "        inputs - Subset of positive and negative examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - An array specifying the importance of each example\n",
    "        \n",
    "    '''     \n",
    "### START GIVEN CODE ###\n",
    "\n",
    "\n",
    "    \n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk through the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        \n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        \n",
    "        # Start from pos_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            # If the positive index goes past the positive dataset lenght,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "### END GIVEN CODE ###\n",
    "            \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "    \n",
    "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            # If the negative index goes past the negative dataset length,\n",
    "            if neg_index >= len_data_neg:\n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment neg_index by one\n",
    "            neg_index += 1\n",
    "\n",
    "### END CODE HERE ###        \n",
    "\n",
    "### START GIVEN CODE ###\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Update the start index for positive data \n",
    "        # so that it's n_to_take positions after the current pos_index\n",
    "        pos_index += n_to_take\n",
    "        \n",
    "        # Update the start index for negative data \n",
    "        # so that it's n_to_take positions after the current neg_index\n",
    "        neg_index += n_to_take\n",
    "        \n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "### END GIVEN CODE ###\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = [0] * n_pad\n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            \n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = [1]*n_to_take\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of zeros)\n",
    "        # The length is the number of negative examples in the batch\n",
    "        target_neg = [0]*n_to_take\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "### GIVEN CODE ###\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1afc197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[2005 4450 3200    9    0    0    0    0    0    0    0]\n",
      " [4953  567 2000 1454 5173 3498  141 3498  130  459    9]\n",
      " [3760  109  136  583 2929 3968    0    0    0    0    0]\n",
      " [ 250 3760    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "def train_generator(batch_size, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5e04637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs shape is (4, 14)\n",
      "The targets shape is (4,)\n",
      "The example weights shape is (4,)\n",
      "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
      "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
      "input tensor: [5737 2900 3760    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 858  256 3651 5738  307 4457  567 1230 2766  328 1202 3760    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generator\n",
    "\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective targets)\n",
    "tmp_data_gen = train_generator(batch_size = 4)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "print(f\"The targets shape is {tmp_targets.shape}\")\n",
    "print(f\"The example weights shape is {tmp_example_weights.shape}\")\n",
    "\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10425a4",
   "metadata": {},
   "source": [
    "### Part3: Defining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9c3b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b65a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Relu\n",
    "class Relu(Layer):\n",
    "    \"\"\"Relu activation function implementation\"\"\"\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input: \n",
    "            - x (a numpy array): the input\n",
    "        Output:\n",
    "            - activation (numpy array): all positive or 0 version of x\n",
    "        '''\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        activation = np.maximum(x,0)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fba3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data is:\n",
      "[[-2. -1.  0.]\n",
      " [ 0.  1.  2.]]\n",
      "Output of Relu is:\n",
      "[[0. 0. 0.]\n",
      " [0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Test your relu function\n",
    "x = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)\n",
    "relu_layer = Relu()\n",
    "print(\"Test data is:\")\n",
    "print(x)\n",
    "print(\"Output of Relu is:\")\n",
    "print(relu_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ec86f",
   "metadata": {},
   "source": [
    "### Dense class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b421833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the fastmath module within trax\n",
    "from trax import fastmath\n",
    "\n",
    "# use the numpy module from trax\n",
    "np = fastmath.numpy\n",
    "\n",
    "# use the fastmath.random module from trax\n",
    "random = fastmath.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86b40c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed generated by random.get_prng\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1], dtype=uint32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose a matrix with 2 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix generated with a normal distribution with mean 0 and stdev of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.95730704, -0.96992904,  1.0070664 ],\n",
       "             [ 0.36619025,  0.17294823,  0.29092228]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See how the fastmath.trax.random.normal function works\n",
    "tmp_key = random.get_prng(seed=1)\n",
    "print(\"The random seed generated by random.get_prng\")\n",
    "display(tmp_key)\n",
    "\n",
    "print(\"choose a matrix with 2 rows and 3 columns\")\n",
    "tmp_shape=(2,3)\n",
    "display(tmp_shape)\n",
    "\n",
    "# Generate a weight matrix\n",
    "# Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow\n",
    "# Just avoid setting the dtype and allow it to use the default data type\n",
    "tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)\n",
    "\n",
    "print(\"Weight matrix generated with a normal distribution with mean 0 and stdev of 1\")\n",
    "display(tmp_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54c4028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Dense\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    A dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # __init__ is implemented for you\n",
    "    def __init__(self, n_units, init_stdev=0.1):\n",
    "        \n",
    "        # Set the number of units in this layer\n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = init_stdev\n",
    "\n",
    "    # Please implement 'forward()'\n",
    "    def forward(self, x):\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Matrix multiply x and the weight matrix\n",
    "        dense = np.dot(x, self.weights) \n",
    "        \n",
    "### END CODE HERE ###\n",
    "        return dense\n",
    "\n",
    "    # init_weights\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # The input_signature has a .shape attribute that gives the shape as a tuple\n",
    "        input_shape = input_signature.shape\n",
    "\n",
    "        # Generate the weight matrix from a normal distribution, \n",
    "        # and standard deviation of 'stdev'  \n",
    "        w = self._init_stdev * random.normal(\n",
    "            key = random_key, shape = (input_shape[-1], self._n_units))\n",
    "        \n",
    "### END CODE HERE ###     \n",
    "        self.weights = w\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62facc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are\n",
      "  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126\n",
      "  -0.04265672  0.0986188  -0.05575325  0.00153249]\n",
      " [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617\n",
      "  -0.03237354  0.16234995  0.02450038 -0.13809784]\n",
      " [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459\n",
      "  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]\n",
      "Foward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209\n",
      "  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing your Dense layer \n",
    "dense_layer = Dense(n_units=10)  #sets  number of units in dense layer\n",
    "random_key = random.get_prng(seed=0)  # sets random seed\n",
    "z = np.array([[2.0, 7.0, 25.0]]) # input array \n",
    "\n",
    "dense_layer.init(z, random_key)\n",
    "print(\"Weights are\\n \",dense_layer.weights) #Returns randomly generated weights\n",
    "print(\"Foward function output is \", dense_layer(z)) # Returns multiplied values of units and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b775e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "592dda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### using tl.Dense & tl.Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bbcb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tl.Dense)\n",
    "# help(tl.Serial)\n",
    "# help(tl.Embedding)\n",
    "# help(tl.mean)\n",
    "# help(tl.LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77e4f4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding_3_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_embed = tl.Embedding(vocab_size=3, d_feature=2)\n",
    "display(tmp_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b250da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 0 creates a vector whose length equals the vocabulary size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2.5, 3.5, 4.5], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 5.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pretend the embedding matrix uses \n",
    "# 2 elements for embedding the meaning of a word\n",
    "# and has a vocabulary size of 3\n",
    "# So it has shape (2,3)\n",
    "tmp_embed = np.array([[1,2,3,],\n",
    "                    [4,5,6]\n",
    "                   ])\n",
    "\n",
    "# take the mean along axis 0\n",
    "print(\"The mean along axis 0 creates a vector whose length equals the vocabulary size\")\n",
    "display(np.mean(tmp_embed,axis=0))\n",
    "\n",
    "print(\"The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\")\n",
    "display(np.mean(tmp_embed,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0203166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: classifier\n",
    "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
    "        \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # create embedding layer\n",
    "    embed_layer = tl.Embedding(\n",
    "        vocab_size=vocab_size, # Size of the vocabulary\n",
    "        d_feature=embedding_dim)  # Embedding dimension\n",
    "    \n",
    "    # Create a mean layer, to create an \"average\" word embedding\n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    \n",
    "    # Create a dense layer, one unit for each output\n",
    "    dense_output_layer = tl.Dense(n_units =  output_dim)\n",
    "\n",
    "    \n",
    "    # Create the log softmax layer (no parameters needed)\n",
    "    log_softmax_layer = tl.LogSoftmax()\n",
    "    \n",
    "    # Use tl.Serial to combine all layers\n",
    "    # and create the classifier\n",
    "    # of type trax.layers.combinators.Serial\n",
    "    model = tl.Serial(\n",
    "      embed_layer, # embedding layer\n",
    "      mean_layer, # mean layer\n",
    "      dense_output_layer, # dense output layer \n",
    "      log_softmax_layer # log softmax layer\n",
    "    )\n",
    "### END CODE HERE ###     \n",
    "    \n",
    "    # return the model of type\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4a68f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trax.layers.combinators.Serial'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_9087_256\n",
       "  Mean\n",
       "  Dense_2\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_model = classifier()\n",
    "\n",
    "print(type(tmp_model))\n",
    "display(tmp_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06398fa",
   "metadata": {},
   "source": [
    "### Part4: Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8b062ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### using trax.supervised.training.Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3efc5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(trax.supervised.training.TrainTask)\n",
    "# help(trax.supervised.training.EvalTask)\n",
    "# help(trax.supervised.training.Loop)\n",
    "# help(trax.optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73cb66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "batch_size = 16\n",
    "rnd.seed(271)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    n_steps_per_checkpoint=10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")\n",
    "\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "615885eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/junsheng/model/\n"
     ]
    }
   ],
   "source": [
    "output_dir = '~/model/'\n",
    "output_dir_expand = os.path.expanduser(output_dir)\n",
    "print(output_dir_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd4aacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    '''\n",
    "    Input: \n",
    "        classifier - the model you are building\n",
    "        train_task - Training task\n",
    "        eval_task - Evaluation task\n",
    "        n_steps - the evaluation steps\n",
    "        output_dir - folder to save your files\n",
    "    Output:\n",
    "        trainer -  trax trainer\n",
    "    '''\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    training_loop = training.Loop(\n",
    "                                classifier, # The learning model\n",
    "                                train_task, # The training task\n",
    "                                eval_task = eval_task, # The evaluation task\n",
    "                                output_dir=output_dir) # The output directory\n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "### END CODE HERE ###\n",
    "\n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f0054b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    '''\n",
    "    Input: \n",
    "        classifier - the model you are building\n",
    "        train_task - Training task\n",
    "        eval_task - Evaluation task\n",
    "        n_steps - the evaluation steps\n",
    "        output_dir - folder to save your files\n",
    "    Output:\n",
    "        trainer -  trax trainer\n",
    "    '''\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    training_loop = training.Loop(\n",
    "                                classifier,  # The learning model\n",
    "                                train_task,  # The training task\n",
    "                                eval_tasks=[eval_task], # The evaluation task\n",
    "                                output_dir = output_dir) # The output directory\n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "### END CODE HERE ###\n",
    "\n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfffd42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junsheng/anaconda3/lib/python3.8/site-packages/jax/lib/xla_bridge.py:355: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "/home/junsheng/anaconda3/lib/python3.8/site-packages/jax/lib/xla_bridge.py:368: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    110: Ran 10 train steps in 12.86 secs\n",
      "Step    110: train CrossEntropyLoss |  0.00391775\n",
      "Step    110: eval  CrossEntropyLoss |  0.00095890\n",
      "Step    110: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    120: Ran 10 train steps in 6.37 secs\n",
      "Step    120: train CrossEntropyLoss |  0.01067365\n",
      "Step    120: eval  CrossEntropyLoss |  0.00388715\n",
      "Step    120: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    130: Ran 10 train steps in 4.22 secs\n",
      "Step    130: train CrossEntropyLoss |  0.00577395\n",
      "Step    130: eval  CrossEntropyLoss |  0.02079256\n",
      "Step    130: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    140: Ran 10 train steps in 2.72 secs\n",
      "Step    140: train CrossEntropyLoss |  0.03044287\n",
      "Step    140: eval  CrossEntropyLoss |  0.02165309\n",
      "Step    140: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    150: Ran 10 train steps in 1.23 secs\n",
      "Step    150: train CrossEntropyLoss |  0.02176759\n",
      "Step    150: eval  CrossEntropyLoss |  0.00361435\n",
      "Step    150: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    160: Ran 10 train steps in 2.78 secs\n",
      "Step    160: train CrossEntropyLoss |  0.00361290\n",
      "Step    160: eval  CrossEntropyLoss |  0.22500116\n",
      "Step    160: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    170: Ran 10 train steps in 1.19 secs\n",
      "Step    170: train CrossEntropyLoss |  0.03342548\n",
      "Step    170: eval  CrossEntropyLoss |  0.02406415\n",
      "Step    170: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    180: Ran 10 train steps in 1.15 secs\n",
      "Step    180: train CrossEntropyLoss |  0.01977936\n",
      "Step    180: eval  CrossEntropyLoss |  0.00143982\n",
      "Step    180: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    190: Ran 10 train steps in 1.04 secs\n",
      "Step    190: train CrossEntropyLoss |  0.00161178\n",
      "Step    190: eval  CrossEntropyLoss |  0.00046062\n",
      "Step    190: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    200: Ran 10 train steps in 1.19 secs\n",
      "Step    200: train CrossEntropyLoss |  0.00134429\n",
      "Step    200: eval  CrossEntropyLoss |  0.00042951\n",
      "Step    200: eval          Accuracy |  1.00000000\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20474c13",
   "metadata": {},
   "source": [
    "### Practice Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52922e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
      "The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)\n",
      "The shape of the labels is (16,), which is the batch size.\n",
      "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
     ]
    }
   ],
   "source": [
    "# Create a generator object\n",
    "tmp_train_generator = train_generator(16)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a38f4f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
      "Column 0 is the probability of a negative sentiment (class 0)\n",
      "Column 1 is the probability of a positive sentiment (class 1)\n",
      "\n",
      "View the prediction array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.0194388e+01, -3.7193298e-05],\n",
       "             [-7.2248535e+00, -7.2860718e-04],\n",
       "             [-9.9221802e+00, -4.9114227e-05],\n",
       "             [-1.0227901e+01, -3.6239624e-05],\n",
       "             [-7.4947591e+00, -5.5623055e-04],\n",
       "             [-9.6691628e+00, -6.2942505e-05],\n",
       "             [-9.5990095e+00, -6.7710876e-05],\n",
       "             [-3.8492775e+00, -2.1525145e-02],\n",
       "             [-1.0848045e-03, -6.8270092e+00],\n",
       "             [-2.6845932e-04, -8.2223110e+00],\n",
       "             [-1.1110306e-04, -9.1060047e+00],\n",
       "             [ 0.0000000e+00, -1.7969788e+01],\n",
       "             [-6.4055920e-03, -5.0538015e+00],\n",
       "             [-3.1137466e-04, -8.0741367e+00],\n",
       "             [-5.1760674e-04, -7.5663810e+00],\n",
       "             [-9.2506409e-05, -9.2861271e+00]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad854622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg log prob -10.1944\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -7.2249\tPos log prob -0.0007\t is positive? True\t actual 1\n",
      "Neg log prob -9.9222\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -10.2279\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -7.4948\tPos log prob -0.0006\t is positive? True\t actual 1\n",
      "Neg log prob -9.6692\tPos log prob -0.0001\t is positive? True\t actual 1\n",
      "Neg log prob -9.5990\tPos log prob -0.0001\t is positive? True\t actual 1\n",
      "Neg log prob -3.8493\tPos log prob -0.0215\t is positive? True\t actual 1\n",
      "Neg log prob -0.0011\tPos log prob -6.8270\t is positive? False\t actual 0\n",
      "Neg log prob -0.0003\tPos log prob -8.2223\t is positive? False\t actual 0\n",
      "Neg log prob -0.0001\tPos log prob -9.1060\t is positive? False\t actual 0\n",
      "Neg log prob 0.0000\tPos log prob -17.9698\t is positive? False\t actual 0\n",
      "Neg log prob -0.0064\tPos log prob -5.0538\t is positive? False\t actual 0\n",
      "Neg log prob -0.0003\tPos log prob -8.0741\t is positive? False\t actual 0\n",
      "Neg log prob -0.0005\tPos log prob -7.5664\t is positive? False\t actual 0\n",
      "Neg log prob -0.0001\tPos log prob -9.2861\t is positive? False\t actual 0\n"
     ]
    }
   ],
   "source": [
    "# turn probabilites into category predictions\n",
    "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d356a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the array of booleans\n",
    "print(\"Array of booleans\")\n",
    "display(tmp_is_positive)\n",
    "\n",
    "# convert boolean to type int32\n",
    "# True is converted to 1\n",
    "# False is converted to 0\n",
    "tmp_is_positive_int = tmp_is_positive.astype(np.int32)\n",
    "\n",
    "\n",
    "# View the array of integers\n",
    "print(\"Array of integers\")\n",
    "display(tmp_is_positive_int)\n",
    "\n",
    "# convert boolean to type float32\n",
    "tmp_is_positive_float = tmp_is_positive.astype(np.float32)\n",
    "\n",
    "# View the array of floats\n",
    "print(\"Array of floats\")\n",
    "display(tmp_is_positive_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72ebfd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9957c",
   "metadata": {},
   "source": [
    "### Part5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26aa2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(preds, y, y_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        preds: a tensor of shape (dim_batch, output_dim) \n",
    "        y: a tensor of shape (dim_batch, output_dim) with the true labels\n",
    "        y_weights: a n.ndarray with the a weight for each example\n",
    "    Output: \n",
    "        accuracy: a float between 0-1 \n",
    "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
    "        sum_weights (np.float32): Sum of the weights\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Create an array of booleans, \n",
    "    # True if the probability of positive sentiment is greater than\n",
    "    # the probability of negative sentiment\n",
    "    # else False\n",
    "    is_pos =  preds[:,1] > preds[:,0]\n",
    "\n",
    "    # convert the array of booleans into an array of np.int32\n",
    "    is_pos_int = is_pos.astype(np.int32)\n",
    "    \n",
    "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
    "    correct = (is_pos_int == y)\n",
    "\n",
    "    # Count the sum of the weights.\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    \n",
    "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
    "    correct_float = correct.astype(np.float32)\n",
    "    \n",
    "    # Multiply each prediction with its corresponding weight.\n",
    "    weighted_correct_float = correct_float * y_weights\n",
    "\n",
    "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
    "    # denominator.\n",
    "    weighted_num_correct = np.sum(weighted_correct_float)\n",
    " \n",
    "    # Divide the number of weighted correct predictions by the sum of the\n",
    "    # weights.\n",
    "    accuracy = weighted_num_correct  / sum_weights\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdc2c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's prediction accuracy on a single training batch is: 100.0%\n",
      "Weighted number of correct predictions 64.0; weighted number of total observations predicted 64\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "tmp_val_generator = val_generator(64)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_val_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "\n",
    "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
    "\n",
    "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
    "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076e4cf",
   "metadata": {},
   "source": [
    "### Testing your model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9f2ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch has dimensions (X, Y, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a36084f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "def test_model(generator, model):\n",
    "    '''\n",
    "    Input: \n",
    "        generator: an iterator instance that provides batches of inputs and targets\n",
    "        model: a model instance \n",
    "    Output: \n",
    "        accuracy: float corresponding to the accuracy\n",
    "    '''\n",
    "    \n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    for batch in generator: \n",
    "        \n",
    "        # Retrieve the inputs from the batch\n",
    "        inputs = batch[0]\n",
    "        \n",
    "        # Retrieve the targets (actual labels) from the batch\n",
    "        targets = batch[1]\n",
    "        \n",
    "        # Retrieve the example weight.\n",
    "        example_weight = batch[2]\n",
    "\n",
    "        # Make predictions using the inputs\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)\n",
    "        \n",
    "        # Update the total number of correct predictions\n",
    "        # by adding the number of correct predictions from this batch\n",
    "        total_num_correct += batch_num_correct\n",
    "        \n",
    "        # Update the total number of predictions \n",
    "        # by adding the number of predictions made for the batch\n",
    "        total_num_pred += batch_num_pred\n",
    "\n",
    "    # Calculate accuracy over all examples\n",
    "    accuracy = total_num_correct / total_num_pred\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bcb1f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model on the validation set is 0.9940\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# testing the accuracy of your model: this takes around 20 seconds\n",
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3beefcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used to predict on your own sentnece\n",
    "def predict(sentence):\n",
    "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    \n",
    "    # Batch size 1, add dimension for batch, to work with the model\n",
    "    inputs = inputs[None, :]  \n",
    "    \n",
    "    # predict with the model\n",
    "    preds_probs = model(inputs)\n",
    "    \n",
    "    # Turn probabilities into categories\n",
    "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
    "    \n",
    "    sentiment = \"negative\"\n",
    "    if preds == 1:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    return preds, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c12d1145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the sentence \n",
      "***\n",
      "\"I feel sad at the first place, but it turns out to be a great day becuase I received lots of gifts\"\n",
      "***\n",
      "is positive.\n",
      "\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"I would rather stay at home all day instead of go out at this type of weather\"\n",
      "***\n",
      "is negative.\n"
     ]
    }
   ],
   "source": [
    "# try a positive sentence\n",
    "sentence = \"I feel sad at the first place, but it turns out to be a great day becuase I received lots of gifts\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "# try a negative sentence\n",
    "sentence = \"I would rather stay at home all day instead of go out at this type of weather\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70a0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ea8bd1bf4f75da5143adadd71660e28625523cf80bc3249ef6623003bbd93c5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
