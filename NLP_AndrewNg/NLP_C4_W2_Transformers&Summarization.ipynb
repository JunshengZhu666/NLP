{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36906d0d",
   "metadata": {},
   "source": [
    "# Lecture 1: The Three ways of Attention and Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caad1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec14f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    \"\"\"Create tensor from list of lists\"\"\"\n",
    "    return np.array(t)\n",
    "\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \"\"\"Display shape and tensor\"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07efad64",
   "metadata": {},
   "source": [
    "\n",
    "Before implementing it yourself, you can play around with a toy example of dot product attention without the softmax operation. Technically it would not be dot product attention without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.\n",
    "\n",
    "The formula for attention is this one:\n",
    "\n",
    "$$ \\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\ $$\n",
    "$d_{k}$ stands for the dimension of queries and keys.\n",
    "\n",
    "The query, key, value and mask vectors are provided for this example.\n",
    "\n",
    "Notice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a1aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567cd1ce",
   "metadata": {},
   "source": [
    "Instructions: Implement the dot product attention. Concretely, implement the following equation\n",
    "\n",
    "$$ \\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\ $$\n",
    "$Q$ - query, $K$ - key, $V$ - values, $M$ - mask, ${d_k}$ - depth/dimension of the queries and keys (used for scaling down)\n",
    "\n",
    "You can implement this formula either by trax numpy (trax.math.numpy) or regular numpy but it is recommended to use jnp.\n",
    "\n",
    "Something to take into consideration is that within trax, the masks are tensors of True/False values not 0's and $-\\infty$ as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use jnp.where() and treat the mask as a tensor of boolean values with False for values that need to be masked and True for the ones that don't.\n",
    "\n",
    "Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as @ for dot product or .T for transposing. Use jnp.matmul() and jnp.swapaxes() instead.\n",
    "\n",
    "This is the self-attention block for the transformer decoder. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15a974",
   "metadata": {},
   "source": [
    "### Dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fb228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProductAttention(query, key, value, mask, scale=True):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (numpy.ndarray): array of query representations with shape (L_q by d)\n",
    "        key (numpy.ndarray): array of key representations with shape (L_k by d)\n",
    "        value (numpy.ndarray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (numpy.ndarray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "        scale (bool): whether to scale the dot product of the query and transposed key\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1],\"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    if scale:\n",
    "        depth = query.shape[-1]\n",
    "    else: \n",
    "        depth = 1\n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth)\n",
    "    \n",
    "    # Apply the mask\n",
    "    if mask is not None: \n",
    "        dots = np.where(mask, dots, np.full_like(dots, -1e9))\n",
    "    \n",
    "    # Softmax formula implementation\n",
    "    # Use scipy.special.logsumexp of masked_qkT to avoid underflow by division by large numbers\n",
    "    # Note: softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = scipy.special.logsumexp(dots, axis = -1, keepdims = True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use np.exp()\n",
    "    dots = np.exp(dots - logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use np.matmul()\n",
    "    attention = np.matmul(dots, value)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c342d",
   "metadata": {},
   "source": [
    "### Mask the dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d07721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_self_attention(q, k, v, scale=True):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    Args:\n",
    "        q (numpy.ndarray): queries.\n",
    "        k (numpy.ndarray): keys.\n",
    "        v (numpy.ndarray): values.\n",
    "    Returns:\n",
    "        numpy.ndarray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size of the penultimate dimension of the query\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Use np.tril() - Lower triangle of an array and np.ones()\n",
    "    mask = np.tril(np.ones((1, mask_size, mask_size), dtype = np.bool_), k = 0)\n",
    "        \n",
    "    return DotProductAttention(q, k, v, mask, scale = scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9329cd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 1.        , 0.        ],\n",
       "        [0.84967455, 0.15032545, 0.84967455]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_self_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b70184",
   "metadata": {},
   "source": [
    "# Lecture 2: The transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e4fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gin\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b9a63",
   "metadata": {},
   "source": [
    "### Embedding sentence with positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f04b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\n",
    "    \"\"\"Returns a list of layers that: \n",
    "    1. takes a block of text as input, \n",
    "    2. embeds the words in that text, and \n",
    "    3. adds positional encoding, \n",
    "       i.e. associates a number in range(max_len) with \n",
    "       each word in each sentence of embedded input text \n",
    "    \n",
    "    The input is a list of tokenized blocks of text\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train' or 'eval'.\n",
    "    \"\"\"\n",
    "    # Embedding inputs and positional encoder\n",
    "    return [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate = dropout, mode = mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len = max_len, mode = mode)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0ec91",
   "metadata": {},
   "source": [
    "### Multi-head causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd3838",
   "metadata": {},
   "source": [
    "### Feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2515db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a feed-forward block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    return [ \n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_ff),\n",
    "        # Add activation function passed in as a parameter (you need to call it!)\n",
    "        ff_activation(),  # Generally ReLU\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate = dropout, mode = mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_model),\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate = dropout, mode = mode)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd73026",
   "metadata": {},
   "source": [
    "### Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b6fc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
    "    return [\n",
    "        tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.LayerNorm(),\n",
    "          # Add causal attention \n",
    "          tl.CausalAttention(d_feature, n_heads = n_heads, dropout = dropout, mode = mode)\n",
    "        ),\n",
    "        tl.Residaul(\n",
    "          # Add feed-forward block\n",
    "          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\n",
    "            FeedForward(d_model, d_ff, dropout, mode, ff_activation)\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a72f46",
   "metadata": {},
   "source": [
    "### The final transformer decoder: repeating N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b894d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "    \"\"\"Returns a Transformer language model.\n",
    "\n",
    "    The input to the model is a tensor of tokens. (This model uses only the\n",
    "    decoder part of the overall Transformer.)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [\n",
    "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(mode = mode),\n",
    "        # Add embedding inputs and positional encoder\n",
    "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
    "        tl.Dense(vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3c988",
   "metadata": {},
   "source": [
    "# Assignment2: Transformer Summarizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "244b00ec",
   "metadata": {},
   "source": [
    "Introduction\n",
    "Part 1: Importing the dataset\n",
    "    1.1 Encode & Decode helper functions\n",
    "    1.2 Defining parameters\n",
    "    1.3 Exploring the data\n",
    "Part 2: Summarization with transformer\n",
    "    2.1 Dot product attention\n",
    "        Exercise 01\n",
    "    2.2 Causal Attention\n",
    "        Exercise 02\n",
    "    2.3 Transformer decoder block\n",
    "        Exercise 03\n",
    "    2.4 Transformer Language model\n",
    "        Exercise 04\n",
    "Part 3: Training\n",
    "    3.1 Training the model\n",
    "        Exercise 05\n",
    "Part 4: Evaluation\n",
    "    4.1 Loading in a trained model\n",
    "Part 5: Testing with your own input\n",
    "        Exercise 6\n",
    "5.1 Greedy decoding\n",
    "        Exercise 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "283ab789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2692a976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 558.32 MiB (download: 558.32 MiB, generated: 1.27 GiB, total: 1.82 GiB) to data/cnn_dailymail/3.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e49e880edc4e32b34d25d186f14235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e286543648344de48561f88cdf5fc5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3075a3de6afa4680a00218759610a335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling cnn_dailymail-train.tfrecord...:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling cnn_dailymail-validation.tfrecord...:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling cnn_dailymail-test.tfrecord...:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cnn_dailymail downloaded and prepared to data/cnn_dailymail/3.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This will download the dataset if no data_dir is specified.\n",
    "# Downloading and processing can take bit of time,\n",
    "# so we have the data already in 'data/' for you\n",
    "\n",
    "# Importing CNN/DailyMail articles dataset\n",
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf337a3",
   "metadata": {},
   "source": [
    "### 1.1 Tokenize & Detokenize helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c173065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  \n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "  \n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309522b",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing for language Models: Concatenate It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a886ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens \n",
    "SEP = 0 # SEparetor token Padding\n",
    "EOS = 1 # End Of Sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint  = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        # accounting for EOS and SEP\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1)\n",
    "        yield joint, joint, np.array(mask)\n",
    "        \n",
    "# combine the preprocessing steps into a pipeline like this\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # tokenize\n",
    "    trax.data.Tokenize(vocab_dir = 'vocab_dir/',\n",
    "                        vocab_file = 'summarize32k.subword.subwords'),\n",
    "    # use function defined\n",
    "    preprocess, \n",
    "    # filters out examples to long\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to data streams\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask  = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71699563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1e23f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " Kabul, Afghanistan (CNN) -- Thousands of bottles of alcohol were\n",
      "destroyed in Kabul this week, in what authorities described as the\n",
      "product of a crackdown on illegal smuggling and sales. The bottles\n",
      "were confiscated over a two-year period in and around the Afghan\n",
      "capital, according to Kabul police and criminal investigations chief\n",
      "Mohammad Zahir. They were taken almost exclusively from \"Afghan\n",
      "sources and not foreigners,\" he said. The illicit items were being\n",
      "stored by Afghan customs officials, who burned the bottles Wednesday\n",
      "after receiving authorization from the city's attorney general's\n",
      "office, he added. Alcohol is largely banned in Afghanistan, and its\n",
      "sales and consumption considered a criminal offense for the country's\n",
      ". Muslims, who constitute roughly 99% of the population. Certain areas\n",
      "that cater to foreigners, however, are permitted to sell it. Zahir\n",
      "said that it was in these areas -- mostly international hotels -- that\n",
      "local sellers had come into possession of the alcohol. CNN's Matiullah\n",
      "Mati contributed to this report .<EOS><pad>Official: Bottles are\n",
      "almost exclusively from \"Afghan sources\" and not foreigners . Alcohol\n",
      "is largely banned in Afghanistan . Certain areas, however, that cater\n",
      "to foreigners are permitted to sell it .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15717dc4",
   "metadata": {},
   "source": [
    "### 1.3 Batching with bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc580945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing to create batched generators.\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes.\n",
    "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n",
    "# 4 of length < 512. And so on. \n",
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,    8,    4,    2, 1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd97512f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1254)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every execution will result in generation of a different article\n",
    "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
    "input_batch, _, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# Shape of the input_batch\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64ad3e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  202 14607  1352     9   119   262  2713   676  3697    28  3713 17179\n",
      "   883   132   772  1492    11   397 14172   163   852   669 27634     4\n",
      "  3188 27634    98  2713  5772  3808   527   852  3213   627    63   186\n",
      "   133   329 20100     5    63    95  6402  1380     6  5367     4  6765\n",
      "   133   132   213  9160   527   213  4874   527    72  2021   220   984\n",
      "     3    52   149   320  1151    28   621  5078   320  5268  6765  1133\n",
      "   391    52     7     5    19   285    89  5457    23  7903   196  1133\n",
      "   391    52  9272     7    26  1133   391  1452   186   322    18   452\n",
      "  5736  2685  1740   132   213    94  4921   714     2   647   103     7\n",
      "     5   132 10451  5057   181  9716     5   222    80  9808     5  1376\n",
      "  1133   669   391 16517    18 18830  4108   626   186  2622   254   209\n",
      "   171 14664   686   186  5437 15585 17893    21    28 12841     4   132\n",
      "   163 23932     2   181  3522 20068 17238     4 10642    17    15   138\n",
      "   123  1864 10470  9596  1074     3     9  1607   592   229   213  2620\n",
      "   238    11   213  2372   186   299   564     3 17745  5712     2   102\n",
      "   213   772  9508 14424  6765     2    13  3891   285    13  2935   320\n",
      "   669 27634     4 20554    75    78   116    75   159   132    28  2498\n",
      "  7597  4617 27634   391  1743  2845   213   669 27634     4  9009   864\n",
      "  4172 27634   391 17745  5712    13  6646   285   669 27634     4  7511\n",
      "    13  4185     2   103   229   416   320  1151    28 16131 10707   527\n",
      "  5367     5 13710   132  4172  6053 27634   391  7540    20   231  6995\n",
      "   391   223    13 12554    17   285   592     2    13   439  2886   864\n",
      "   809   130  2278  1133   391    34  1640     2   536     2    77  1353\n",
      "    92  3698   427   181    14     6  3072   427   181   669 27634     4\n",
      "   852  4617 27634   391   413   320   362   527   103     2  7511   213\n",
      " 10451   260 24541   408    64  1248    50     8  1446  2627 27634     4\n",
      "   242 19890   391  4698     4  2713  4617 27634   391  2522   161   139\n",
      "  5057  1133   391  1593   146    25 12510    17     2   527   468  1133\n",
      "   391   200  1376    78   163   709   141   358     7    26  1755   320\n",
      "  2586   213   164  1281  3188   775   527   299   564   592     3   198\n",
      "  1353   579   139 10701 25760 21820  2685 11648   593  3157    78    28\n",
      " 22847  1563     6   967 21109  5133  9225     3   244  7511    28  9745\n",
      "  1337   107 11270    84  5735 13391   132  1640   285    22    40   320\n",
      "  3977    28  1091  1019 11771    16   403   196     2   186  3405    68\n",
      "   132    15  4449  2197  2035   391  5103  1613   161  5057  1133   391\n",
      "   449     7     5    28   196  6082  1928    74   213   388  8744  1510\n",
      "    87  3454     7     5  8867   854 24010  2842    78  3698     3   577\n",
      "     2 11648  2622   852   229    19  2685  7270 11611   181   525     6\n",
      "  4575   103   229     3   871   773   229    19   412  1954   132  9434\n",
      "   299   564 12776   275   412   103   229   132  9434  1648   166 30462\n",
      "     4   647   213  3681   476   103   181    19 30462     4    51   456\n",
      "   358     7    26  1006   412   227  2685  9434 15274  8157     5  1133\n",
      "   391   129    39 20146   114  2322   213   440  2622   527  9445  1648\n",
      "     2   188   122    51   423    31   126 20685  7814   940     3  9885\n",
      " 14101  6353     2   487     2    51  3067   213   314   809   101   890\n",
      "    86  2856 19456  9301   527     2   476     2    28 17446   966 16994\n",
      "  7883   186   690   320    28  5417     4     6 20581  3868  1133   391\n",
      "    52  1063   285   122    87 11446 26383  6197  2976   320 26012    78\n",
      "   213  2372     2    22   170  1151   217   320   340   320    28  7462\n",
      "  5708  3653   527   163  2212   322   517   320  8072    15   181    68\n",
      "   593   691  9420   103   229   107   669 27634     4  5057  4172 27634\n",
      "   391   449   641 13582  4558 18269 19209   114  1248   213   660   171\n",
      "   213  2826   812   132    28   347   316  2730 12040     4   473     3\n",
      "   257   366  1133   391    34  2730 12040     4     2   163   669 27634\n",
      "     4 11279    16 20067 27634   391   149  4108   593    78  3004   320\n",
      " 26012  2685    15   938     2   163 10542   235   186   163 13151  3056\n",
      "     3  2198   213   758  9081     2   213   924   255  4609   647    28\n",
      "   669 27634     4  3188 27634   391  2193  4676   527   213  9429     7\n",
      "     5  2212  8857   320 13862     4     2   181   122   103   229   811\n",
      "   320   344   285    28   669 27634     4  4346   537 27634   391    62\n",
      "  2929   213  1602   412 11648  1133   391   223   213   924  3615  2193\n",
      "  4676   527  2212  8857     2   285  1166    62  1151    28   765  8960\n",
      "     2   186    44 18393   320 19664  1133   391   200   188   155   213\n",
      "   669 27634     4  4346   537 27634   391  1291 21784   691 13021     5\n",
      "     2   213 11439    81   527   213   911   255  1194   105   412    28\n",
      "  3188     3  7702   824   229 11406   299   564    23    44  3188   775\n",
      "     2   186   103     7     5    19   166   103    49  1591   125  1931\n",
      "    11  1780   564   229    44 11648  1019    50   775   320  1591    36\n",
      "   680   537 30462     4    35  1151  2881  1613   691   125   469  1133\n",
      "   391  7702   285     7     5   213   744     3    34   285   929     2\n",
      "   299   564  6765  1435   256   107    28  2082   788     2   186    44\n",
      "   107    28 17500  4361  2067   922  1471    78 12680 14382  2067  1019\n",
      "   469   320  2177     2   181    28   922 16381  9290 20755  3910  1209\n",
      "   691   109  1320 16545     8   412  1248  4738     4 12620    24  1780\n",
      "   564  3213   627  1435     8  1180    12    19  1019  3276     2   186\n",
      "    41  1435 21707   691   213  6358  1847  1133   391    34   285   929\n",
      "    41  1006    44   107   213 22931 21815    17  3234   527   432  8857\n",
      "     3   187 15337   412    51   170  1151   527   440  2622   132   626\n",
      "     2  3980   103     7     5    55   320  3067   213   314   809   299\n",
      "   564  1133   391  1486   457     2    13     7    75 19184    21   132\n",
      "    28   138   285  1454   213 14415 11402  1435    19  1133   391    13\n",
      "     7    75    78   299   564     2   186   213 12554     5  2215   809\n",
      "   156  1838    55   320    55  1435     2   953     7     5   476     2\n",
      "  2170 11648   186   403  7648  4090   320 14574     4   265     3  7702\n",
      "   213 14415 11402    62  1151   256 23147   320 12361    14   213  5057\n",
      "   527 17710  8339     4     8   412  2454   240  7926  1086   206  2627\n",
      "   391  1248   213 23361 24010  2842   527 21566  3759 26805   183 12554\n",
      "   185   122    41    40    44   776   482  1248   299   564     3     9\n",
      "  9508 14424 13780     5    28   671   917  1133   391   791   229   103\n",
      "   320   288   213  1607   111    28  1023  3188     2   213   126   527\n",
      "   163  1690     2   181    87 16233     4 26493   236   852    98     9\n",
      "  2826   812   108  2586   213 22437   330  8965   527    89    55     2\n",
      "    35  1349  1248   109   925   373     6   104     6   292     2    41\n",
      "  1435 11728 23177     5  1133   391    13     7   183   369   553    28\n",
      " 19545  8500   132    28  2826   812  2155     2   186    13  5585  3326\n",
      "   213 14415 11402  3263    28   621   527    55    78 26767  1133   391\n",
      " 11818  3854   412    41  1435  1248 10595  6765    78   299   564     2\n",
      "    39    31 20017  8784  2155  3102    28   617  1192   527  7270   101\n",
      "  9615 30462     4   181 13862     4 30462     4   852    98  1473   122\n",
      "    41   124     2   691   146   213  2680    39  1151  2341   213   382\n",
      "   827 10734  2104     1     0 12325  3457  4930 10793    11  4188  9202\n",
      "     5    95 12554     5 11648  9508 14424  3606   739   527  7511  2622\n",
      "   229  3188 16346 27439  6774  1628    69   465  2826   812   320  7450\n",
      "     4   132  1132     3  7702   103     7     5    55   320  3067  3188\n",
      "   314   809   299   564  2104     1]\n"
     ]
    }
   ],
   "source": [
    "# print corresponding integer values\n",
    "print(input_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd7f5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\n",
      " (CNN)The New York Police Department faced a newly pressing question in\n",
      "recent weeks: What constitutes an online \"threat\"? Police reviewed\n",
      "hundreds of online postings--and made several arrests--over alleged\n",
      "anti-cop threats made in the wake of the killing of two officers last\n",
      "month. It used to be a lot easier to define threats.  It's not that\n",
      "our dialogue has evolved much.  It hasn't.  Art and music have always\n",
      "spoken about violence in the most graphic terms, whether it's in rap\n",
      "lyrics or Guns N' Roses songs.   Kids have consumed violent art and\n",
      "speech since long before Hansel and Gretel baked a witch in an oven,\n",
      "or Beowulf hacked his way through Southern Scandinavia. The difference\n",
      "today is the delivery system: the Internet and social media. Suppose,\n",
      "after the recent NYPD threats, I posted that I planned to \"swarm on\n",
      "any m... in a blue uniform,\" meaning specifically the \"punk police.\"\n",
      "Suppose I promise that \"when I finish, it is going to be a bloodbath\n",
      "of cops dyin.'\" Scary right?  If I tweeted that today, I might expect\n",
      "police at my door.  In 1988, though, there was no Twitter ... or\n",
      "e-mail ... or \"online,\" come to think of it, when the rap group NWA\n",
      "came out with its (famous) \"F*** Tha Police,\" containing those very\n",
      "lyrics.  People then were offended, of course.  But songs on an album\n",
      "just don't seem to contain the same direct threat potential of social\n",
      "media today. There was something very nonspecific about threatening\n",
      "language contained on a commercially mass-produced cassette tape. And\n",
      "when a megastar like Axl Rose sang in 1988 that he had to kill a woman\n",
      "for complaining too much, and bury her in his backyard,  millions\n",
      "heard those lyrics.  That's a much bigger audience than the 13\n",
      "followers reading some guy's rabid musings on Twitter. So, threatening\n",
      "speech online is not about how viral or far-reaching it is. Our\n",
      "society is not as interested in protecting social media rants as it is\n",
      "in protecting artists because—whether the courts say it or not—we\n",
      "really don't feel as good about protecting dummies.  We will\n",
      "vigorously protect the free speech of legitimate artists, even if we\n",
      "find their work repulsive. Instinctively, however, we draw the line at\n",
      "people whose only creative accomplishments consist of, say, a bitter\n",
      "child custody dispute and access to a Wi-Fi signal.  It seems that if\n",
      "some shmendrik wants to rant on the Internet, he should be able to\n",
      "point to a scintilla of an actual music career to defend his or her\n",
      "language by claiming it is like \"lyrics.\" That analysis dovetails\n",
      "nicely with the issues before the Supreme Court in a case called\n",
      "Elonis v. United States.  In Elonis, an \"aspiring rapper\" used violent\n",
      "language on Facebook to rant about his wife, an elementary school and\n",
      "an FBI agent. Under the First Amendment, the court must decide whether\n",
      "a \"threat\" requires proof of the defendant's actual intent to\n",
      "threaten, or if it is enough to show that a \"reasonable person\" would\n",
      "regard the statement as threatening.  If the court ultimately requires\n",
      "proof of actual intent, that test would be a higher burden, and more\n",
      "favorable to defendants.  But even under the \"reasonable person\"\n",
      "standard favored by prosecutors, the hearer of the words must\n",
      "understand them as a threat. Maybe this is why social media has more\n",
      "threat potential, and it's not because it can reach many persons:\n",
      "Social media is more threatening for its potential to reach one\n",
      "specific person—but be overheard by many others.  Maybe that's the\n",
      "key. In that sense, social media threats are less like a musical\n",
      "performance, and more like a menacing phone call placed on\n",
      "speakerphone for others to hear, or a call surreptitiously recorded by\n",
      "your Russian girlfriend (as with Mel Gibson). Social media postings\n",
      "are (generally) not for profit, and they are authored by the speaker\n",
      "alone.  In that sense they feel more like the unfiltered statements of\n",
      "present intent. As protective as we should be of free speech in art,\n",
      "maybe it's time to draw the line at social media.  Then again, I'm\n",
      "biased in a way that perhaps the justices are not.  I'm on social\n",
      "media, and the tweets directed at me from time to time are, let's say,\n",
      "pretty threatening and too profane to reprint here. Maybe the justices\n",
      "would be less inclined to equate the lyrics of Eminem (as Justice John\n",
      "Roberts recently did)  with the poetic musings of frighteningly\n",
      "abusive tweeters if they had more personal experience with social\n",
      "media. The NYPD confronts a similar problem.  How is it to know the\n",
      "difference between a true threat, the work of an artist, or some troll\n",
      "sounding off online? The Supreme Court may contain the brightest minds\n",
      "of our time, but compared with your average 14-year-old, they are\n",
      "Luddites.  I've never seen a hashtag in a Supreme Court opinion, and I\n",
      "seriously doubt the justices spend a lot of time on Instagram.  Tasked\n",
      "as they are with defining threats on social media, will their reasoned\n",
      "opinion reflect a current understanding of how people communicate—or\n",
      "threaten—online? Even if they do, by then the kids will be onto the\n",
      "next thing anyway .<EOS><pad>DannyCevallos: Arrests over tweets\n",
      "threatening NYPD raise issue of when speech is threat . He says\n",
      "Supreme Court to weigh in soon. Maybe it's time to draw threat line at\n",
      "social media .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# print the article and its summary\n",
    "print('Article:\\n\\n', detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68acd9a",
   "metadata": {},
   "source": [
    "# Part2: Summarization with transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeacb2b",
   "metadata": {},
   "source": [
    "### Dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60c4a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tensor(t):\n",
    "    \"\"\"Create tensor from list of lists\"\"\"\n",
    "    return jnp.array(t)\n",
    "\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \"\"\"Display shape and tensor\"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ec13c3",
   "metadata": {},
   "source": [
    "The formula for attention is this one:\n",
    "\n",
    "$$ \\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\ $$\n",
    "$d_{k}$ stands for the dimension of queries and keys.\n",
    "\n",
    "The query, key, value and mask vectors are provided for this example.\n",
    "\n",
    "Notice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef04b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39635af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query dot key shape: (2, 2)\n",
      "\n",
      "[[0.57735026 2.309401  ]\n",
      " [1.1547005  2.8867514 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
    "display_tensor(q_dot_k, 'query dot key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6b35430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key shape: (2, 2)\n",
      "\n",
      "[[ 5.7735026e-01  2.3094010e+00]\n",
      " [-1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked = q_dot_k + m\n",
    "display_tensor(masked, 'masked query dot key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5843a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key dot value shape: (2, 3)\n",
      "\n",
      "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
      " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(masked @ v, 'masked query dot key dot value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac9ebde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "key with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "\n",
      "value with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[0 1 0]\n",
      "  [1 0 1]]]\n",
      "\n",
      "boolean mask shape: (2, 2)\n",
      "\n",
      "[[ True  True]\n",
      " [False  True]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_with_batch = q[None,:]\n",
    "display_tensor(q_with_batch, 'query with batch dim')\n",
    "k_with_batch = k[None,:]\n",
    "display_tensor(k_with_batch, 'key with batch dim')\n",
    "v_with_batch = v[None,:]\n",
    "display_tensor(v_with_batch, 'value with batch dim')\n",
    "m_bool = create_tensor([[True, True], [False, True]])\n",
    "display_tensor(m_bool, 'boolean mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "281a1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 01\n",
    "\n",
    "### In mask, tensors are boolean values, use jnp.where\n",
    "\n",
    "### Use jnp.maymul() and jnp.swapaxes()  for @ and .T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ff049",
   "metadata": {},
   "source": [
    "$$ \\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\ $$\n",
    "$Q$ - query,  $K$ - key,  $V$ - values,  $M$ - mask,  ${d_k}$ - depth/dimension of the queries and keys (used for scaling down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f165734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: DotProductAttention\n",
    "def DotProductAttention(query, key, value, mask):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
    "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
    "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    depth = query.shape[-1]\n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
    "    \n",
    "    # Apply the mask\n",
    "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "    \n",
    "    # Softmax formula implementation\n",
    "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
    "    # Hint: Last axis should be used and keepdims should be True\n",
    "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis = -1, keepdims = True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use jnp.exp()\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use jnp.matmul()\n",
    "    attention = jnp.matmul(dots, value)\n",
    "\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbea00f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
       "              [1.        , 0.        , 1.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678e00e",
   "metadata": {},
   "source": [
    "### 2.2 Casual Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5f32a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802ed87",
   "metadata": {},
   "source": [
    "Exercise 02\n",
    "Implement the following functions that will be needed for Causal Attention:\n",
    "\n",
    "compute_attention_heads : Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
    "dot_product_self_attention : Creates a mask matrix with False values above the diagonal and True values below and calls DotProductAttention which implements dot product self attention.\n",
    "compute_attention_output : Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads.\n",
    "Next there are some toy tensors which may serve to give you an idea of the data shapes and opperations involved in Causal Attention. They are also useful to test out your functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24aaf196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query matrix (2D tensor) shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
      "\n",
      "[[[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]\n",
      "\n",
      "\n",
      " [[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]]\n",
      "\n",
      "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor2d = create_tensor(q)\n",
    "display_tensor(tensor2d, 'query matrix (2D tensor)')\n",
    "\n",
    "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
    "display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n",
    "\n",
    "tensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n",
    "\n",
    "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1509580",
   "metadata": {},
   "source": [
    "compute_attention_heads : Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15108d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_attention_heads_closure\n",
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_heads function\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\" Compute the attention heads.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Size of the x's batch dimension\n",
    "        batch_size = x.shape[0]\n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head) )\n",
    "        # Transpose x using jnp.transpose()\n",
    "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
    "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
    "        x = jnp.reshape(x, (-1, seqlen, d_head))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7935d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "output tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(tensor3dc3b, \"input tensor\")\n",
    "result_cah = compute_attention_heads_closure(2,3)(tensor3dc3b)\n",
    "display_tensor(result_cah, \"output tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9b4fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: dot_product_self_attention\n",
    "def dot_product_self_attention(q, k, v):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    Args:\n",
    "        q (jax.interpreters.xla.DeviceArray): queries.\n",
    "        k (jax.interpreters.xla.DeviceArray): keys.\n",
    "        v (jax.interpreters.xla.DeviceArray): values.\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
    "    # NOTE: there is a revision underway with the autograder to tolerate better indexing. \n",
    "    # Until then, please index q.shape using negative values (this is equivalent to counting from right to left)\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
    "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1b54cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.        , 1.        , 0.        ],\n",
       "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "defb1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: compute_attention_output_closure\n",
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_output function\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \"\"\" Compute the attention output.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[0]\n",
    "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
    "        x = jnp.reshape(x, (-1, n_heads, seqlen, d_head))\n",
    "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Reshape to allow to concatenate the heads\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78a7db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "output tensor shape: (1, 6, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]\n",
      "  [1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]\n",
      "  [1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(result_cah, \"input tensor\")\n",
    "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
    "display_tensor(result_cao, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aeb7cd",
   "metadata": {},
   "source": [
    "Instructions: Implement the causal attention. Your model returns the causal attention through a $tl.Serial$ with the following:\n",
    "\n",
    "[tl.Branch](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch) : consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.\n",
    "[tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn): Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.\n",
    "[tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn): Takes in restack_attention_heads to allow for parallel computing.\n",
    "[tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense): Final Dense layer, with dimension d_feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da078dea",
   "metadata": {},
   "source": [
    "### Causal Attention Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a639ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: CausalAttention\n",
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "    \"\"\"Transformer-style multi-headed causal attention.\n",
    "\n",
    "    Args:\n",
    "        d_feature (int):  dimensionality of feature embedding.\n",
    "        n_heads (int): number of attention heads.\n",
    "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
    "        dot_product_self_attention (function): dot_product_self_attention function. \n",
    "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
    "        mode (str): 'train' or 'eval'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
    "    # Since you are dealing with closures you might need to call the outer \n",
    "    # function with the correct parameters to get the actual uncalled function.\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
    "        \n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
    "        # Since you are dealing with closures you might need to call the outer \n",
    "        # function with the correct parameters to get the actual uncalled function.\n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
    "        tl.Dense(d_feature) # Final dense layer\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6701d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Branch_out3[\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "  ]\n",
      "  DotProductAttn_in3\n",
      "  AttnOutput\n",
      "  Dense_512\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the causal attention model\n",
    "print(CausalAttention(d_feature=512, n_heads=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf36ed",
   "metadata": {},
   "source": [
    "### 2.3 Transformer decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ce40b",
   "metadata": {},
   "source": [
    "[tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) : used to layer normalize\n",
    "[tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) : the dense layer\n",
    "[ff_activation](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) : feed forward activation (we use ReLu) here.\n",
    "[tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) : dropout layer\n",
    "[tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) : dense layer\n",
    "[tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) : dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION: DecoderBlock\n",
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Create masked multi-head attention block using CausalAttention function\n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    feed_forward = [ \n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_ff),\n",
    "        # Add activation function passed in as a parameter (you need to call it!)\n",
    "        ff_activation(), # Generally ReLU\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_model),\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout,mode=mode)\n",
    "    ]\n",
    "\n",
    "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.LayerNorm(),\n",
    "          # Add causal attention block previously defined (without parentheses)\n",
    "          causal_attention,\n",
    "          # Add dropout with rate and mode specified\n",
    "          tl.Dropout(rate=dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          # Add feed forward block (without parentheses)\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c438857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the decoder block\n",
    "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153948de",
   "metadata": {},
   "source": [
    "Exercise 04\n",
    "Instructions: Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need.\n",
    "\n",
    "positional_enconder - a list containing the following layers:\n",
    "\n",
    "tl.Embedding\n",
    "tl.Dropout\n",
    "tl.PositionalEncoding\n",
    "A list of n_layers decoder blocks.\n",
    "\n",
    "[tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial): takes in the following layers or lists of layers:\n",
    "[tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): : shift the tensor to the right by padding on axis 1.\n",
    "positional_encoder : encodes the text positions.\n",
    "decoder_blocks : the ones you created.\n",
    "[tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) : a layer norm.\n",
    "[tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) : takes in the vocab_size.\n",
    "[tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) : to predict.\n",
    "Go go go!! You can do it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ade90e",
   "metadata": {},
   "source": [
    "### 2.4 Transformer Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb613c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UNQ_C7\n",
    "# GRADED FUNCTION: TransformerLM\n",
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "    \"\"\"Returns a Transformer language model.\n",
    "\n",
    "    The input to the model is a tensor of tokens. (This model uses only the\n",
    "    decoder part of the overall Transformer.)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Embedding inputs and positional encoder\n",
    "    positional_encoder = [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads,\n",
    "                    dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(mode=mode), # Specify the mode!\n",
    "        # Add positional encoder\n",
    "        positional_encoder,\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
    "        tl.Dense(vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddbaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
