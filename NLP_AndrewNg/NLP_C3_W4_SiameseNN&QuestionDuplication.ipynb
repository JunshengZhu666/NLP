{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeca7493",
   "metadata": {},
   "source": [
    "# Lecture 1: Creating a Simese model using Trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4b6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax \n",
    "from trax import layers as tl\n",
    "import trax.fastmath.numpy as np\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f386a",
   "metadata": {},
   "source": [
    "### L2 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ec2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x / np.sqrt(np.sum(x*x, axis = -1, keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125415a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor is of type: <class 'numpy.ndarray'>\n",
      "\n",
      "And looks like this:\n",
      "\n",
      " [[0.68535982 0.95339335 0.00394827 0.51219226 0.81262096]\n",
      " [0.61252607 0.72175532 0.29187607 0.91777412 0.71457578]]\n"
     ]
    }
   ],
   "source": [
    "tensor = numpy.random.random((2,5))\n",
    "print(f'The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c243f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized tensor is of type: <class 'jaxlib.xla_extension.DeviceArray'>\n",
      "\n",
      "And looks like this:\n",
      "\n",
      " [[0.45177674 0.6284596  0.00260263 0.33762783 0.5356649 ]\n",
      " [0.40091467 0.47240815 0.1910407  0.6007077  0.46770892]]\n"
     ]
    }
   ],
   "source": [
    "norm_tensor = normalize(tensor)\n",
    "print(f'The normalized tensor is of type: {type(norm_tensor)}\\n\\nAnd looks like this:\\n\\n {norm_tensor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa8da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "model_dimension = 128\n",
    "\n",
    "# Define the LSTM model\n",
    "LSTM = tl.Serial(\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n",
    "        tl.LSTM(model_dimension),\n",
    "        tl.Mean(axis=1),\n",
    "        tl.Fn('Normalize', lambda x: normalize(x))\n",
    "    )\n",
    "\n",
    "# Use the Parallel combinator to create a Siamese model out of the LSTM \n",
    "Siamese = tl.Parallel(LSTM, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba89ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese model:\n",
      "\n",
      "Total layers: 2\n",
      "\n",
      "========\n",
      "Parallel.sublayers_0: Serial[\n",
      "  Embedding_500_128\n",
      "  LSTM_128\n",
      "  Mean\n",
      "  Normalize\n",
      "]\n",
      "\n",
      "========\n",
      "Parallel.sublayers_1: Serial[\n",
      "  Embedding_500_128\n",
      "  LSTM_128\n",
      "  Mean\n",
      "  Normalize\n",
      "]\n",
      "\n",
      "Detail of LSTM models:\n",
      "\n",
      "Total layers: 4\n",
      "\n",
      "========\n",
      "Serial.sublayers_0: Embedding_500_128\n",
      "\n",
      "========\n",
      "Serial.sublayers_1: LSTM_128\n",
      "\n",
      "========\n",
      "Serial.sublayers_2: Mean\n",
      "\n",
      "========\n",
      "Serial.sublayers_3: Normalize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_layers(model, layer_prefix):\n",
    "    print(f\"Total layers: {len(model.sublayers)}\\n\")\n",
    "    for i in range(len(model.sublayers)):\n",
    "        print('========')\n",
    "        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n",
    "\n",
    "print('Siamese model:\\n')\n",
    "show_layers(Siamese, 'Parallel.sublayers')\n",
    "\n",
    "print('Detail of LSTM models:\\n')\n",
    "show_layers(LSTM, 'Serial.sublayers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f07ba",
   "metadata": {},
   "source": [
    "# Lecture 2； Modified Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e3417",
   "metadata": {},
   "source": [
    "This is the original triplet loss function:\n",
    "\n",
    "$\\mathcal{L_\\mathrm{Original}} = \\max{(\\mathrm{s}(A,N) -\\mathrm{s}(A,P) +\\alpha, 0)}$\n",
    "\n",
    "It can be improved by including the mean negative and the closest negative, to create a new full loss function. The inputs are the Anchor $\\mathrm{A}$, Positive $\\mathrm{P}$ and Negative $\\mathrm{N}$.\n",
    "\n",
    "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P) +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P) +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$\n",
    "\n",
    "Let me show you what that means exactly, and how to calculate each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687280f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab71346",
   "metadata": {},
   "source": [
    "### cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f45ea0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- input -- \n",
      "v1 : [1. 2. 3.]\n",
      "v2 : [1.  2.  3.5] \n",
      "\n",
      "-- Outputs --\n",
      "cosine similarity : 0.9974086507360697\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "print(\" -- input -- \")\n",
    "v1 = np.array([1, 2,  3], dtype = float)\n",
    "v2 = np.array([1, 2, 3.5])\n",
    "\n",
    "# different v2\n",
    "# v2 = v1\n",
    "# v2 = v1 * -1\n",
    "# v2 = np.array([0, -42, 1])\n",
    "\n",
    "print(\"v1 :\", v1)\n",
    "print(\"v2 :\", v2, \"\\n\")\n",
    "\n",
    "# Similarity score\n",
    "def cosine_similarity(v1, v2):\n",
    "    numerator = np.dot(v1, v2)\n",
    "    denominator = np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2))\n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"-- Outputs --\")\n",
    "print(\"cosine similarity :\", cosine_similarity(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7435f654",
   "metadata": {},
   "source": [
    "### two ways to compute similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "845b7940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Inputs --\n",
      "v1 :\n",
      "[[ 1  2  3]\n",
      " [ 9  8  7]\n",
      " [-1 -4 -2]\n",
      " [ 1 -7  2]] \n",
      "\n",
      "v2 :\n",
      "[[ 0.45750402  3.22640837  2.46536562]\n",
      " [ 7.90138197  8.26541659  6.04771597]\n",
      " [ 1.61694616 -3.60997344 -1.19958002]\n",
      " [ 0.32473533 -4.48705547  0.536061  ]] \n",
      "\n",
      "batch sizes match : True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two batches of vectors example\n",
    "# Input data\n",
    "print(\"-- Inputs --\")\n",
    "v1_1 = np.array([1, 2, 3])\n",
    "v1_2 = np.array([9, 8, 7])\n",
    "v1_3 = np.array([-1, -4, -2])\n",
    "v1_4 = np.array([1, -7, 2])\n",
    "v1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\n",
    "print(\"v1 :\")\n",
    "print(v1, \"\\n\")\n",
    "v2_1 = v1_1 + np.random.normal(0, 2, 3)  # add some noise to create approximate duplicate\n",
    "v2_2 = v1_2 + np.random.normal(0, 2, 3)\n",
    "v2_3 = v1_3 + np.random.normal(0, 2, 3)\n",
    "v2_4 = v1_4 + np.random.normal(0, 2, 3)\n",
    "v2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\n",
    "print(\"v2 :\")\n",
    "print(v2, \"\\n\")\n",
    "\n",
    "# Batch sizes must match\n",
    "b = len(v1)\n",
    "print(\"batch sizes match :\", b == len(v2), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aadcd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c6fa44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option 1: loop\n",
      "[[ 0.9357224   0.87966125 -0.59496745 -0.41536027]\n",
      " [ 0.82907874  0.99667487 -0.39470942 -0.46306599]\n",
      " [-0.97695837 -0.89508912  0.80362302  0.79719929]\n",
      " [-0.57269902 -0.39830659  0.80618846  0.98537695]] \n",
      "\n",
      "option 2 : vec norm & dot product\n",
      "[[ 0.9357224   0.87966125 -0.59496745 -0.41536027]\n",
      " [ 0.82907874  0.99667487 -0.39470942 -0.46306599]\n",
      " [-0.97695837 -0.89508912  0.80362302  0.79719929]\n",
      " [-0.57269902 -0.39830659  0.80618846  0.98537695]] \n",
      "\n",
      "outputs are the same : True\n"
     ]
    }
   ],
   "source": [
    "# Option1: nested loop \n",
    "sim_1 = np.zeros([b, b])\n",
    "\n",
    "\n",
    "for row in range(0, sim_1.shape[0]):\n",
    "    for col in range(0, sim_1.shape[1]):\n",
    "        sim_1[row, col] = cosine_similarity(v1[row], v2[col])\n",
    "        \n",
    "print(\"option 1: loop\")\n",
    "print(sim_1, \"\\n\")\n",
    "\n",
    "# Option 2 : vector normalization and dot product\n",
    "def norm(x):\n",
    "    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n",
    "\n",
    "sim_2 = np.dot(norm(v1), norm(v2).T)\n",
    "\n",
    "print(\"option 2 : vec norm & dot product\")\n",
    "print(sim_2, \"\\n\")\n",
    "\n",
    "# Check\n",
    "print(\"outputs are the same :\", np.allclose(sim_1, sim_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592df89",
   "metadata": {},
   "source": [
    "### HArd Negetive Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5f3340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Inputs --\n",
      "sim :\n",
      "[[ 0.9 -0.8  0.3 -0.5]\n",
      " [-0.4  0.5  0.1 -0.1]\n",
      " [ 0.3  0.1 -0.4 -0.8]\n",
      " [-0.5 -0.2 -0.7  0.5]]\n",
      "shape : (4, 4) \n",
      "\n",
      "sim_ap: \n",
      "[[ 0.9  0.   0.   0. ]\n",
      " [ 0.   0.5  0.   0. ]\n",
      " [ 0.   0.  -0.4  0. ]\n",
      " [ 0.   0.   0.   0.5]] \n",
      "\n",
      "sim_an: \n",
      "[[ 0.  -0.8  0.3 -0.5]\n",
      " [-0.4  0.   0.1 -0.1]\n",
      " [ 0.3  0.1  0.  -0.8]\n",
      " [-0.5 -0.2 -0.7  0. ]] \n",
      "\n",
      " -- Output -- \n",
      "mean_neg: \n",
      "[[-0.33333333]\n",
      " [-0.13333333]\n",
      " [-0.13333333]\n",
      " [-0.46666667]] \n",
      "\n",
      "closest_neg :\n",
      "[[ 0.3]\n",
      " [ 0.1]\n",
      " [-0.8]\n",
      " [-0.2]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded matrix of similarity scores\n",
    "sim_hardcoded = np.array(\n",
    "    [\n",
    "        [0.9, -0.8, 0.3, -0.5],\n",
    "        [-0.4, 0.5, 0.1, -0.1],\n",
    "        [0.3, 0.1, -0.4, -0.8],\n",
    "        [-0.5, -0.2, -0.7, 0.5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sim = sim_hardcoded\n",
    "### START CODE HERE ###\n",
    "# Try using different values for the matrix of similarity scores\n",
    "# sim = 2 * np.random.random_sample((b,b)) -1   # random similarity scores between -1 and 1\n",
    "# sim = sim_2                                   # the matrix calculated previously\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Batch size\n",
    "b = sim.shape[0]\n",
    "\n",
    "print(\"-- Inputs --\")\n",
    "print(\"sim :\")\n",
    "print(sim)\n",
    "print(\"shape :\", sim.shape, \"\\n\")\n",
    "\n",
    "# Positives\n",
    "# s(A, P)\n",
    "sim_ap = np.diag(sim)\n",
    "print(\"sim_ap: \")\n",
    "print(np.diag(sim_ap), \"\\n\")\n",
    "\n",
    "# Negatives\n",
    "# all the s(A, N)\n",
    "sim_an = sim - np.diag(sim_ap)\n",
    "print(\"sim_an: \")\n",
    "print(sim_an, \"\\n\")\n",
    "\n",
    "\n",
    "print(\" -- Output -- \")\n",
    "# mean negative\n",
    "mean_neg = np.sum(sim_an, axis = 1, keepdims = True) / (b - 1)\n",
    "print(\"mean_neg: \")\n",
    "print(mean_neg, \"\\n\")\n",
    "\n",
    "# Closest negative\n",
    "# Max s(A,N) that is <= s(A,P) for each row\n",
    "mask_1 = np.identity(b) == 1            # mask to exclude the diagonal\n",
    "mask_2 = sim_an > sim_ap.reshape(b, 1)  # mask to exclude sim_an > sim_ap\n",
    "mask = mask_1 | mask_2\n",
    "sim_an_masked = np.copy(sim_an)         # create a copy to preserve sim_an\n",
    "sim_an_masked[mask] = -2\n",
    "\n",
    "closest_neg = np.max(sim_an_masked, axis=1, keepdims=True)\n",
    "print(\"closest_neg :\")\n",
    "print(closest_neg, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6b79d",
   "metadata": {},
   "source": [
    "### The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4362bfb",
   "metadata": {},
   "source": [
    "$\\mathrm{A}$, Positive $\\mathrm{P}$ and Negative $\\mathrm{N}$.\n",
    "\n",
    "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P) +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P) +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e80ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Outputs --\n",
      "loss full :\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.51666667]\n",
      " [0.        ]] \n",
      "\n",
      "cost : 0.517\n"
     ]
    }
   ],
   "source": [
    "# alpha\n",
    "alpha = 0.25\n",
    "\n",
    "# loss 1\n",
    "l_1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
    "\n",
    "# loss 2\n",
    "l_2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
    "\n",
    "# loss full\n",
    "l_full = l_1 + l_2\n",
    "\n",
    "# cost\n",
    "cost = np.sum(l_full)\n",
    "\n",
    "print(\"-- Outputs --\")\n",
    "print(\"loss full :\")\n",
    "print(l_full, \"\\n\")\n",
    "print(\"cost :\", \"{:.3f}\".format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af277d",
   "metadata": {},
   "source": [
    "# Lecture 3: Evaluate a Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a61671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax.fastmath.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ded62",
   "metadata": {},
   "source": [
    "Elements\n",
    "\n",
    "q1: vector with dimension (batch_size X max_length) containing first questions to compare in the test set.\n",
    "q2: vector with dimension (batch_size X max_length) containing second questions to compare in the test set.\n",
    "\n",
    "Notice that for each pair of vectors within a batch $([q1_1, q1_2, q1_3, ...]$, $[q2_1, q2_2,q2_3, ...])$  $q1_i$ is associated to $q2_k$.\n",
    "\n",
    "y_test: 1 if  $q1_i$ and $q2_k$ are duplicates, 0 otherwise.\n",
    "\n",
    "v1: output vector from the model's prediction associated with the first questions.\n",
    "\n",
    "v2: output vector from the model's prediction associated with the second questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "255a6291",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'q1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8b6f3ab7556c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q1.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'q1 has shape: {q1.shape} \\n\\nAnd it looks like this: \\n\\n {q1}\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'q1.npy'"
     ]
    }
   ],
   "source": [
    "q1 = np.load('q1.npy')\n",
    "print(f'q1 has shape: {q1.shape} \\n\\nAnd it looks like this: \\n\\n {q1}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b4b0b",
   "metadata": {},
   "source": [
    "# Assginment 4: Question duplicates"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd3314bf",
   "metadata": {},
   "source": [
    "Learn about Siamese networks\n",
    "Understand how the triplet loss works\n",
    "Understand how to evaluate accuracy\n",
    "Use cosine similarity between the model's outputted vectors\n",
    "Use the data generator to get batches of questions\n",
    "Predict using your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006be1a9",
   "metadata": {},
   "source": [
    "# Part1: IMporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "from trax.fastmath import numpy as fastnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4659cd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question pairs:  404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"questions.csv\")\n",
    "N=len(data)\n",
    "print('Number of question pairs: ', N)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0792c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a train set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e55bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 10240\n"
     ]
    }
   ],
   "source": [
    "N_train = 300000\n",
    "N_test  = 10*1024\n",
    "data_train = data[:N_train]\n",
    "data_test  = data[N_train:N_train+N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
    "del(data) # remove to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "065b2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use the duplicate questions to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d12f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate questions:  111486\n",
      "indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
     ]
    }
   ],
   "source": [
    "td_index = (data_train['is_duplicate'] == True).to_numpy()\n",
    "td_index = [i for i, x in enumerate(td_index) if x]\n",
    "print('number of duplicate questions: ', len(td_index))\n",
    "print('indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73d6e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][5])  #  Example of question duplicates (first one in data)\n",
    "print(data_train['question2'][5])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d05268",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train_words = np.array(data_train['question1'][td_index])\n",
    "Q2_train_words = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test_words = np.array(data_test['question1'])\n",
    "Q2_test_words = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30cecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How do I prepare for interviews for cse?\n",
      "Question 2:  What is the best way to prepare for cse? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train_words[0])\n",
    "print('Question 2: ', Q2_train_words[0], '\\n')\n",
    "print('Question 1: ', Q1_train_words[5])\n",
    "print('Question 2: ', Q2_train_words[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test_words[0])\n",
    "print('Question 2: ', Q2_test_words[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614865a4",
   "metadata": {},
   "source": [
    "### create a index for each duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11546cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1, assgin 0 for OOV words\n",
    "\n",
    "#2, encode other words with indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34c1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create arrays\n",
    "Q1_train = np.empty_like(Q1_train_words)\n",
    "Q2_train = np.empty_like(Q2_train_words)\n",
    "\n",
    "Q1_test = np.empty_like(Q1_test_words)\n",
    "Q2_test = np.empty_like(Q2_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8511de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is:  36268\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary with the train set\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "vocab = defaultdict(lambda: 0)\n",
    "vocab['<PAD>'] = 1\n",
    "\n",
    "for idx in range(len(Q1_train_words)):\n",
    "    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n",
    "    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n",
    "    q = Q1_train[idx] + Q2_train[idx]\n",
    "    for word in q:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "\n",
    "print('The length of the vocabulary is: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d34dfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocab['<PAD>'])\n",
    "print(vocab['Astrology'])\n",
    "print(vocab['Astronomy'])  #not in vocabulary, returns 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217ceac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(Q1_test_words)): \n",
    "    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n",
    "    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d9e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has reduced to:  111486\n",
      "Test set length:  10240\n"
     ]
    }
   ],
   "source": [
    "print('Train set has reduced to: ', len(Q1_train) ) \n",
    "print('Test set length: ', len(Q1_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f6be4",
   "metadata": {},
   "source": [
    "### 1,2 Converting a question to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5a70fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting questions to array of integers\n",
    "for i in range(len(Q1_train)):\n",
    "    Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
    "    Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
    "\n",
    "        \n",
    "for i in range(len(Q1_test)):\n",
    "    Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
    "    Q2_test[i] = [vocab[word] for word in Q2_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6febfa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "[32, 38, 4, 107, 65, 1015, 65, 11509, 21]\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(Q1_train_words[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(Q1_train[0],'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(Q1_test_words[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(Q1_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9cf15",
   "metadata": {},
   "source": [
    "### split the train set into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "557a89fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "The length of the training set is:   89188\n",
      "The length of the validation set is:  22298\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "cut_off = int(len(Q1_train)* 0.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7abd3",
   "metadata": {},
   "source": [
    "Exercise 01\n",
    "Instructions:\n",
    "Implement the data generator below. Here are some things you will need.\n",
    "\n",
    "While true loop.\n",
    "if index >= len_Q1, set the idx to $0$.\n",
    "The generator should return shuffled batches of data. To achieve this without modifying the actual question lists, a list containing the indexes of the questions is created. This list can be shuffled and used to get random batches everytime the index is reset.\n",
    "Append elements of $Q1$ and $Q2$ to input1 and input2 respectively.\n",
    "if len(input1) == batch_size, determine max_len as the longest question in input1 and input2. Ceil max_len to a power of $2$ (for computation purposes) using the following command:  max_len = 2**int(np.ceil(np.log2(max_len))).\n",
    "Pad every question by vocab['<PAD>'] until you get the length max_len.\n",
    "Use yield to return input1, input2.\n",
    "Don't forget to reset input1, input2 to empty arrays at the end (data generator resumes from where it last left)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8925f",
   "metadata": {},
   "source": [
    "### unstanding the iterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c9316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        Q1 (list): List of transformed (to tensor) questions.\n",
    "        Q2 (list): List of transformed (to tensor) questions.\n",
    "        batch_size (int): Number of elements per batch.\n",
    "        pad (int, optional): Pad character from the vocab. Defaults to 1.\n",
    "        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.\n",
    "    Yields:\n",
    "        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
    "        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates\n",
    "              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    input1 = []\n",
    "    input2 = []\n",
    "    idx = 0\n",
    "    len_q = len(Q1)\n",
    "    question_indexes = [*range(len_q)]\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(question_indexes)\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    while True:\n",
    "        if idx >= len_q:\n",
    "            # if idx is greater than or equal to len_q, set idx accordingly \n",
    "            # (Hint: look at the instructions above)\n",
    "            idx = len_q\n",
    "            # shuffle to get random batches if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(question_indexes)\n",
    "        \n",
    "        # get questions at the `question_indexes[idx]` position in Q1 and Q2\n",
    "        q1 = Q1[question_indexes[idx]]\n",
    "        q2 = Q2[question_indexes[idx]]\n",
    "        \n",
    "        # increment idx by 1\n",
    "        idx += 1\n",
    "        # append q1\n",
    "        input1.append(q1)\n",
    "        # append q2\n",
    "        input2.append(q2)\n",
    "        if len(input1) == batch_size:\n",
    "            # determine max_len as the longest question in input1 & input 2\n",
    "            # Hint: use the `max` function. \n",
    "            # take max of input1 & input2 and then max out of the two of them.\n",
    "            max_len = max(max([len(q) for q in input1]),\n",
    "                          max([len(q) for q in input2]))\n",
    "            # pad to power-of-2 (Hint: look at the instructions above)\n",
    "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
    "            b1 = []\n",
    "            b2 = []\n",
    "            for q1, q2 in zip(input1, input2):\n",
    "                # add [pad] to q1 until it reaches max_len\n",
    "                q1 = q1 + [pad] * (max_len - len(q1))\n",
    "                # add [pad] to q2 until it reaches max_len\n",
    "                q2 = q2 + [pad] * (max_len - len(q2))\n",
    "                # append q1\n",
    "                b1.append(q1)\n",
    "                # append q2\n",
    "                b2.append(q2)\n",
    "            # use b1 and b2\n",
    "            yield np.array(b1), np.array(b2)\n",
    "    ### END CODE HERE ###\n",
    "            # reset the batches\n",
    "            input1, input2 = [], []  # reset the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cf9f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        Q1 (list): List of transformed (to tensor) questions.\n",
    "        Q2 (list): List of transformed (to tensor) questions.\n",
    "        batch_size (int): Number of elements per batch.\n",
    "        pad (int, optional): Pad character from the vocab. Defaults to 1.\n",
    "        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.\n",
    "    Yields:\n",
    "        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
    "        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates\n",
    "              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    input1 = []\n",
    "    input2 = []\n",
    "    idx = 0\n",
    "    len_q = len(Q1)\n",
    "    question_indexes = [*range(len_q)]\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(question_indexes)\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    while True:\n",
    "        if idx >= len_q:\n",
    "            # if idx is greater than or equal to len_q, set idx accordingly \n",
    "            # (Hint: look at the instructions above)\n",
    "            idx = len_q\n",
    "            # shuffle to get random batches if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(question_indexes)\n",
    "        \n",
    "        # get questions at the `question_indexes[idx]` position in Q1 and Q2\n",
    "        q1 = Q1[question_indexes[idx]]\n",
    "        q2 = Q2[question_indexes[idx]]\n",
    "        \n",
    "        # increment idx by 1\n",
    "        idx += 1\n",
    "        # append q1\n",
    "        input1.append(q1)\n",
    "        # append q2\n",
    "        input2.append(q2)\n",
    "        if len(input1) == batch_size:\n",
    "            # determine max_len as the longest question in input1 & input 2\n",
    "            # Hint: use the `max` function. \n",
    "            # take max of input1 & input2 and then max out of the two of them.\n",
    "            max_len = max(max([len(q) for q in input1]),\n",
    "                          max([len(q) for q in input2]))\n",
    "            # pad to power-of-2 (Hint: look at the instructions above)\n",
    "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
    "            b1 = []\n",
    "            b2 = []\n",
    "            for q1, q2 in zip(input1, input2):\n",
    "                # add [pad] to q1 until it reaches max_len\n",
    "                q1 = q1 + [pad] * (max_len - len(q1))\n",
    "                q2 = q2 + [pad] * (max_len - len(q2))\n",
    "                # append q1\n",
    "                b1.append(q1)\n",
    "                b2.append(q2)\n",
    "            # use b1 and b2\n",
    "            yield np.array(b1), np.array(b2)\n",
    "    ### END CODE HERE ###\n",
    "            # reset the batches\n",
    "            input1, input2 = [], []  # reset the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39eabc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First questions  :  \n",
      " [[  30   87   78  134 2132 1981   28   78  594   21    1    1    1    1\n",
      "     1    1]\n",
      " [  30   55   78 3541 1460   28   56  253   21    1    1    1    1    1\n",
      "     1    1]] \n",
      "\n",
      "Second questions :  \n",
      " [[  30  156   78  134 2132 9508   21    1    1    1    1    1    1    1\n",
      "     1    1]\n",
      " [  30  156   78 3541 1460  131   56  253   21    1    1    1    1    1\n",
      "     1    1]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\n",
    "print(\"First questions  : \",'\\n', res1, '\\n')\n",
    "print(\"Second questions : \",'\\n', res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d9a79",
   "metadata": {},
   "source": [
    "# Part2: Defining the Siamese model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c122515",
   "metadata": {},
   "source": [
    "You get the question embedding, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally use a triplet loss (explained below) to get the corresponding cosine similarity for each pair of questions. As usual, you will start by importing the data set. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, you are trying to maximize the following.\n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right)$$\n",
    "$A$ is the anchor input, for example $q1_1$, $P$ the duplicate input, for example, $q2_1$, and $N$ the negative input (the non duplicate question), for example $q2_2$.\n",
    "$\\alpha$ is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09671cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Siamese\n",
    "def Siamese(vocab_size=len(vocab), d_model=128, mode='train'):\n",
    "    \"\"\"Returns a Siamese model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).\n",
    "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
    "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to 'train'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Parallel: A Siamese model. \n",
    "    \"\"\"\n",
    "\n",
    "    def normalize(x):  # normalizes the vectors to have L2 norm 1\n",
    "        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    q_processor = tl.Serial(  # Processor will run on Q1 and Q2.\n",
    "        tl.Embedding(vocab_size, d_feature = d_model), # Embedding layer\n",
    "        tl.LSTM(n_units = d_model), # LSTM layer\n",
    "        tl.Mean(axis = 1), # Mean over columns\n",
    "        tl.Fn('Normalize', lambda x: normalize(x))  # Apply normalize function\n",
    "    )  # Returns one vector of shape [batch_size, d_model].\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Run on Q1 and Q2 in parallel.\n",
    "    model = tl.Parallel(q_processor, q_processor)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b732ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel_in2_out2[\n",
      "  Serial[\n",
      "    Embedding_41699_128\n",
      "    LSTM_128\n",
      "    Mean\n",
      "    Normalize\n",
      "  ]\n",
      "  Serial[\n",
      "    Embedding_41699_128\n",
      "    LSTM_128\n",
      "    Mean\n",
      "    Normalize\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# check your model\n",
    "model = Siamese()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71dd09",
   "metadata": {},
   "source": [
    "You will now implement the TripletLoss.\n",
    "As explained in the lecture, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the closest negative. Our loss expression is then:\n",
    "\n",
    "$$\\begin{align} \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P) + mean_{neg} +\\alpha, 0\\right) \\\\ \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P) + closest_{neg} +\\alpha, 0\\right) \\\\ \\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\ \\end{align}$$\n",
    "Further, two sets of instructions are provided. The first set provides a brief description of the task. If that set proves insufficient, a more detailed set can be displayed.\n",
    "\n",
    "\n",
    "Exercise 03\n",
    "Instructions (Brief): Here is a list of things you should do: \n",
    "\n",
    "As this will be run inside trax, use fastnp.xyz when using any xyz numpy function\n",
    "Use fastnp.dot to calculate the similarity matrix $v_1v_2^T$ of dimension batch_size x batch_size\n",
    "Take the score of the duplicates on the diagonal fastnp.diagonal\n",
    "Use the trax functions fastnp.eye and fastnp.maximum for the identity matrix and the maximum.\n",
    "We'll describe the algorithm using a detailed example. Below, V1, V2 are the output of the normalization blocks in our model. Here we will use a batch_size of 4 and a d_model of 3. As explained in lecture, the inputs, Q1, Q2 are arranged so that corresponding inputs are duplicates while non-corresponding entries are not. The outputs will have the same pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc03dff",
   "metadata": {},
   "source": [
    "### Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3a7f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: TripletLossFn\n",
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.\n",
    "        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Triplet Loss.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # use fastnp to take the dot product of the two batches (don't forget to transpose the second argument)\n",
    "    scores = fastnp.dot(v1, v2.T)  # pairwise cosine sim\n",
    "    # calculate new batch size\n",
    "    batch_size = len(scores)\n",
    "    # use fastnp to grab all postive `diagonal` entries in `scores`\n",
    "    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n",
    "    # multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`\n",
    "    negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
    "    # take the row by row `max` of `negative_without_positive`. \n",
    "    # Hint: negative_without_positive.max(axis = [?])  \n",
    "    closest_negative = negative_without_positive.max(axis=1) # [batch]\n",
    "    # subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n",
    "    negative_zero_on_duplicate = scores * (1.0 - fastnp.eye(batch_size))\n",
    "    # use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` \n",
    "    mean_negative = np.sum(negative_zero_on_duplicate, axis=1) / (batch_size-1)\n",
    "    # compute `fastnp.maximum` among 0.0 and `A`\n",
    "    # A = subtract `positive` from `margin` and add `closest_negative` \n",
    "    triplet_loss1 = fastnp.maximum(0.0, margin - positive + closest_negative)\n",
    "    # compute `fastnp.maximum` among 0.0 and `B`\n",
    "    # B = subtract `positive` from `margin` and add `mean_negative`\n",
    "    triplet_loss2 = fastnp.maximum(0.0, margin - positive + mean_negative)\n",
    "    # add the two losses together and take the `fastnp.mean` of it\n",
    "    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0c9e2997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "TripletLossFn(v2,v1)\n",
    "print(\"Triplet Loss:\", TripletLossFn(v2,v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7792b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def TripletLoss(margin=0.25):\n",
    "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
    "    return tl.Fn('TripletLoss', triplet_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d338555",
   "metadata": {},
   "source": [
    "# Part3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4429046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Q1.shape  (89188,)\n",
      "val_Q1.shape    (22298,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab['<PAD>'])\n",
    "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab['<PAD>'])\n",
    "print('train_Q1.shape ', train_Q1.shape)\n",
    "print('val_Q1.shape   ', val_Q1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4219bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7b6224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
    "\n",
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model/'):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        lr_schedule (function): Trax multifactor schedule function.\n",
    "        train_generator (generator, optional): Training generator. Defaults to train_generator.\n",
    "        val_generator (generator, optional): Validation generator. Defaults to val_generator.\n",
    "        output_dir (str, optional): Path to save model to. Defaults to 'model/'.\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.expanduser(output_dir)\n",
    "\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator,         # Use generator (train)\n",
    "        loss_layer=TripletLoss(),             # Use triplet loss. Don't forget to instantiate this object\n",
    "        optimizer=trax.optimizers.Adam(0.01), # Don't forget to add the learning rate parameter\n",
    "        lr_schedule=lr_schedule,              # Use Trax multifactor schedule function\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator,       # Use generator (val)\n",
    "        metrics=[TripletLoss()],          # Use triplet loss. Don't forget to instantiate this object\n",
    "    )\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    training_loop = training.Loop(Siamese(),\n",
    "                                  train_task,\n",
    "                                  eval_tasks=[eval_task],\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb8d980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 5\n",
    "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
    "training_loop.run(train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d630e",
   "metadata": {},
   "source": [
    "# Part4: Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ffcab0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((array([[-0.7434783 , -0.5323    ,  0.26556844, ...,  0.24734499,\n",
       "            0.971258  , -0.3176002 ],\n",
       "          [-1.9103315 , -1.2298064 ,  0.7929189 , ..., -1.3576206 ,\n",
       "           -0.9268899 ,  0.11710498],\n",
       "          [ 1.1356051 ,  1.2533569 ,  1.4670613 , ...,  1.2557949 ,\n",
       "            1.1703947 ,  1.7554839 ],\n",
       "          ...,\n",
       "          [-0.49271938,  0.06522572, -0.74080336, ...,  1.4723355 ,\n",
       "            1.1603701 ,  0.51038134],\n",
       "          [ 1.4607905 , -0.15703319,  0.5001072 , ...,  0.18419997,\n",
       "           -0.5392223 , -0.4307455 ],\n",
       "          [-1.4316021 , -1.2368174 ,  0.0611912 , ..., -0.24021208,\n",
       "            0.34730613, -0.07061554]], dtype=float32),\n",
       "   (((), ((), ())),\n",
       "    ((array([[-0.02550941, -0.06643244, -0.03194237, ..., -0.01651929,\n",
       "              -0.0235158 , -0.02485679],\n",
       "             [-0.0683428 , -0.06671927,  0.0349182 , ...,  0.01044205,\n",
       "               0.02273431,  0.0717328 ],\n",
       "             [-0.03171944,  0.01028203,  0.05781721, ..., -0.04236581,\n",
       "              -0.04657996, -0.06436575],\n",
       "             ...,\n",
       "             [ 0.03127091, -0.02159434, -0.08765983, ...,  0.02809162,\n",
       "              -0.01996687,  0.00975176],\n",
       "             [-0.02458269, -0.02480528, -0.07895178, ..., -0.08660002,\n",
       "               0.05699608, -0.02815895],\n",
       "             [ 0.03601498,  0.04997371,  0.02829743, ..., -0.0378585 ,\n",
       "              -0.08549955, -0.05847359]], dtype=float32),\n",
       "      array([0.9999873 , 0.99998873, 0.9999902 , 0.99998885, 0.9999903 ,\n",
       "             0.99999326, 0.999991  , 0.9999885 , 0.99998873, 0.9999922 ,\n",
       "             0.99999166, 0.9999899 , 0.99998844, 0.9999887 , 0.9999879 ,\n",
       "             0.9999915 , 0.9999861 , 0.9999893 , 0.9999898 , 0.99998873,\n",
       "             0.99998903, 0.9999907 , 0.9999904 , 0.9999898 , 0.9999872 ,\n",
       "             0.9999893 , 0.9999884 , 0.99999195, 0.9999887 , 0.99999106,\n",
       "             0.9999898 , 0.9999901 , 0.99998766, 0.9999907 , 0.9999862 ,\n",
       "             0.999988  , 0.9999887 , 0.9999905 , 0.9999898 , 0.99998724,\n",
       "             0.99998826, 0.99998754, 0.99998873, 0.9999895 , 0.9999895 ,\n",
       "             0.99999183, 0.9999906 , 0.999989  , 0.9999955 , 0.9999871 ,\n",
       "             0.99999   , 0.9999893 , 0.99999106, 0.9999881 , 0.99998856,\n",
       "             0.9999909 , 0.99998885, 0.9999881 , 0.99999106, 0.9999861 ,\n",
       "             0.999991  , 0.9999886 , 0.99999267, 0.9999936 , 0.9999905 ,\n",
       "             0.9999886 , 0.99999076, 0.99998826, 0.99998766, 0.9999887 ,\n",
       "             0.99998975, 0.9999886 , 0.9999902 , 0.9999874 , 0.99998903,\n",
       "             0.9999917 , 0.99998915, 0.99998915, 0.9999961 , 0.99998933,\n",
       "             0.9999876 , 0.99998784, 0.9999894 , 0.999991  , 0.9999883 ,\n",
       "             0.99999297, 0.99999213, 0.9999894 , 0.99998635, 0.9999892 ,\n",
       "             0.9999884 , 0.9999898 , 0.99999076, 0.99999064, 0.99998987,\n",
       "             0.999991  , 0.9999883 , 0.9999901 , 0.9999894 , 0.9999873 ,\n",
       "             0.9999905 , 0.999988  , 0.9999894 , 0.99998724, 0.9999887 ,\n",
       "             0.99998856, 0.9999896 , 0.99999005, 0.9999914 , 0.99999017,\n",
       "             0.9999878 , 0.9999874 , 0.9999894 , 0.9999904 , 0.9999885 ,\n",
       "             0.99999094, 0.9999902 , 0.9999837 , 0.9999906 , 0.99999326,\n",
       "             0.99999076, 0.9999874 , 0.9999915 , 0.99998766, 0.99999183,\n",
       "             0.9999902 , 0.9999897 , 0.999988  , 0.99997586, 0.99998116,\n",
       "             0.99998844, 0.99997693, 0.99998796, 0.9999755 , 0.9999877 ,\n",
       "             0.9999863 , 0.9999866 , 1.0000036 , 0.9999898 , 0.9999837 ,\n",
       "             0.9999796 , 0.99997854, 0.99998003, 0.99999166, 0.99998474,\n",
       "             0.99998194, 0.99998844, 0.9999833 , 0.9999866 , 0.99998385,\n",
       "             0.9999883 , 0.9999821 , 0.99998605, 0.9999878 , 0.9999786 ,\n",
       "             0.99997604, 0.99998504, 0.9999835 , 0.99998707, 0.9999886 ,\n",
       "             0.99998134, 0.9999869 , 0.9999835 , 0.9999783 , 0.99998134,\n",
       "             0.9999805 , 0.9999862 , 0.99998933, 0.99998444, 0.999987  ,\n",
       "             0.99998605, 0.99998814, 0.99999154, 0.99998987, 0.9999869 ,\n",
       "             0.9999892 , 0.9999964 , 0.999978  , 0.99998164, 0.9999928 ,\n",
       "             0.99998844, 0.9999784 , 0.99998164, 0.99998116, 0.99998176,\n",
       "             0.99998647, 0.9999849 , 0.99998003, 0.9999817 , 0.9999905 ,\n",
       "             0.99998236, 0.9999761 , 0.99998266, 0.99999106, 0.9999891 ,\n",
       "             0.9999774 , 0.99998754, 0.99998385, 0.9999859 , 0.9999839 ,\n",
       "             0.9999906 , 0.9999829 , 0.9999904 , 0.9999824 , 0.9999845 ,\n",
       "             0.9999874 , 0.99997836, 0.9999874 , 0.9999776 , 0.999985  ,\n",
       "             0.99999505, 0.9999806 , 0.9999777 , 0.99997616, 0.9999886 ,\n",
       "             0.99998724, 0.9999794 , 0.99998355, 0.9999879 , 0.9999827 ,\n",
       "             0.99999166, 0.9999827 , 0.9999897 , 0.99998856, 0.99998623,\n",
       "             0.9999871 , 0.99998367, 0.9999769 , 0.99998647, 0.9999796 ,\n",
       "             0.99997723, 0.99998975, 0.9999881 , 0.9999898 , 0.9999866 ,\n",
       "             0.99999917, 0.99998224, 0.99998546, 0.9999798 , 0.9999887 ,\n",
       "             0.99999523, 0.9999873 , 0.99998844, 0.99998856, 0.99998164,\n",
       "             0.9999799 , 0.9999893 , 0.99999726, 0.9999856 , 0.9999798 ,\n",
       "             0.9999885 , 0.9999828 , 0.99997884, 0.99997747, 0.99998266,\n",
       "             0.9999813 , 0.9999861 , 0.9999872 , 0.99998856, 0.9999861 ,\n",
       "             0.9999863 , 0.99999833, 0.9999936 , 0.999989  , 0.99999154,\n",
       "             0.9999952 , 0.99999017, 0.99998707, 0.99998784, 0.9999849 ,\n",
       "             0.99998766, 0.99999076, 0.99998665, 0.99999154, 0.99998957,\n",
       "             0.9999874 , 0.9999893 , 0.9999904 , 0.99999046, 0.9999862 ,\n",
       "             0.9999882 , 0.99998885, 0.9999869 , 0.99999475, 0.9999882 ,\n",
       "             0.9999891 , 0.99998987, 0.99998945, 0.9999873 , 0.9999898 ,\n",
       "             0.9999863 , 0.99998856, 0.9999844 , 0.99999213, 0.99999046,\n",
       "             0.9999886 , 0.99998766, 0.9999888 , 0.99998873, 0.9999884 ,\n",
       "             0.999989  , 0.9999942 , 0.99998945, 0.9999929 , 0.9999983 ,\n",
       "             0.9999856 , 0.99998534, 0.99999005, 0.9999897 , 0.9999868 ,\n",
       "             0.9999875 , 0.99998844, 0.99999   , 0.9999888 , 0.99998873,\n",
       "             0.99998474, 0.9999915 , 0.99998856, 0.99999547, 0.9999969 ,\n",
       "             0.99998856, 0.9999887 , 0.99999046, 0.9999899 , 0.99998975,\n",
       "             0.9999856 , 0.99999005, 0.99998456, 0.9999874 , 0.99998575,\n",
       "             0.99999005, 0.99999505, 0.99998885, 0.9999906 , 1.0000011 ,\n",
       "             0.99998915, 0.999983  , 0.99998856, 0.9999981 , 0.9999888 ,\n",
       "             0.9999902 , 0.99999684, 0.99998987, 0.99998665, 0.99998504,\n",
       "             0.9999884 , 0.9999873 , 0.9999863 , 0.9999907 , 0.9999888 ,\n",
       "             0.9999894 , 0.9999893 , 0.9999908 , 0.99999124, 0.9999887 ,\n",
       "             0.9999851 , 0.9999908 , 0.9999859 , 0.9999894 , 0.9999879 ,\n",
       "             0.99998957, 0.9999904 , 0.9999875 , 1.0000012 , 0.999993  ,\n",
       "             0.9999875 , 0.9999866 , 0.9999902 , 0.9999909 , 0.9999873 ,\n",
       "             0.9999889 , 0.99999005, 0.9999866 , 0.99998224, 0.9999889 ,\n",
       "             0.99999666, 0.9999897 , 0.99998766, 0.99999106, 0.9999863 ,\n",
       "             0.9999922 , 0.9999883 , 0.99998796, 0.999989  , 0.99999154,\n",
       "             0.9999906 , 0.9999861 , 0.9999887 , 0.9999845 , 0.9999956 ,\n",
       "             0.99999326, 0.9999852 , 0.99999094, 0.9999919 , 0.9999905 ,\n",
       "             0.9999917 , 0.9999908 , 0.99998504, 0.9999863 , 0.99999243,\n",
       "             0.9999867 , 0.99999386, 0.99998814, 0.99998605, 0.9999883 ,\n",
       "             0.99999666, 0.9999906 , 0.99998415, 0.9999882 , 0.999987  ,\n",
       "             0.9999912 , 0.99999535, 0.99999213, 0.9999912 , 0.9999879 ,\n",
       "             0.9999851 , 0.9999897 , 0.9999888 , 0.99998695, 0.9999934 ,\n",
       "             0.99998665, 0.99999374, 0.99999285, 0.9999873 , 0.99998945,\n",
       "             0.99998707, 0.99998873, 0.9999887 , 0.99998784, 0.99999064,\n",
       "             0.99998647, 0.99999374, 0.9999962 , 0.99998885, 0.9999937 ,\n",
       "             0.99999094, 0.99998695, 0.99998724, 0.99998766, 0.99999255,\n",
       "             0.9999899 , 0.99998575, 0.999988  , 0.9999914 , 0.9999941 ,\n",
       "             0.9999882 , 0.99999523, 0.99999946, 0.9999908 , 0.9999871 ,\n",
       "             0.9999877 , 0.9999923 , 0.99998945, 0.9999867 , 0.9999863 ,\n",
       "             0.99998707, 0.99998933, 0.999985  , 0.999988  , 0.99999607,\n",
       "             0.9999861 , 0.9999903 , 1.0000001 , 0.9999924 , 0.9999875 ,\n",
       "             0.999987  , 0.9999967 , 0.9999887 , 0.9999927 , 0.9999978 ,\n",
       "             0.99999154, 0.99998736, 0.9999856 , 0.9999929 , 0.99998945,\n",
       "             0.9999885 , 0.99998915, 0.9999912 , 0.99998707, 0.9999867 ,\n",
       "             0.99999267, 0.9999922 , 0.9999915 , 0.999992  , 0.9999887 ,\n",
       "             0.9999879 , 0.9999926 , 0.9999883 , 0.9999881 , 0.9999919 ,\n",
       "             0.9999901 , 1.0000008 , 0.9999919 , 0.9999881 , 0.9999875 ,\n",
       "             0.9999878 , 0.99999136, 0.9999868 , 0.99998736, 0.99999213,\n",
       "             0.9999868 , 0.9999882 , 0.9999884 , 0.9999947 , 0.9999933 ,\n",
       "             0.9999927 , 0.9999933 , 0.9999894 , 0.9999923 , 0.9999892 ,\n",
       "             0.99998623, 0.9999848 ], dtype=float32)),),\n",
       "    ()),\n",
       "   (),\n",
       "   ()),\n",
       "  {'__marker_for_cached_weights_': ()}),\n",
       " (((), (((), ((), ())), ((), ()), ()), (), ()),\n",
       "  {'__marker_for_cached_state_': ()}))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in the saved model\n",
    "model = Siamese()\n",
    "model.init_from_file('model.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8267d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "049a4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: classify\n",
    "def classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
    "    \"\"\"Function to test the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        test_Q1 (numpy.ndarray): Array of Q1 questions.\n",
    "        test_Q2 (numpy.ndarray): Array of Q2 questions.\n",
    "        y (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model.\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    for i in range(0, len(test_Q1), batch_size):\n",
    "        # Call the data generator (built in Ex 01) with shuffle=False using next()\n",
    "        # use batch size chuncks of questions as Q1 & Q2 arguments of the data generator. e.g x[i:i + batch_size]\n",
    "        # Hint: use `vocab['<PAD>']` for the `pad` argument of the data generator\n",
    "        q1, q2 = next(data_generator(\n",
    "            test_Q1[i:i + batch_size], test_Q2[i:i + batch_size], batch_size, vocab['<PAD>'], shuffle=False))\n",
    "        # use batch size chuncks of actual output targets (same syntax as example above)\n",
    "        y_test = y[i:i + batch_size]\n",
    "        # Call the model\n",
    "        v1, v2 =model((q1, q2))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            # take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]\n",
    "            # don't forget to transpose the second argument\n",
    "            d = np.dot(v1[j], v2[j].T)\n",
    "            # is d greater than the threshold?\n",
    "            res = d > threshold\n",
    "            # increment accurancy if y_test is equal `res`\n",
    "            accuracy += (y_test[j] == res)\n",
    "    # compute accuracy using accuracy and total length of test questions\n",
    "    accuracy = accuracy / len(test_Q1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54c5ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.3767578125\n"
     ]
    }
   ],
   "source": [
    "# this takes around 1 minute\n",
    "accuracy = classify(Q1_test,Q2_test, y_test, 0.7, model, vocab, batch_size = 512) \n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c14bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part5: Testing with your own questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: predict\n",
    "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
    "    \"\"\"Function for predicting if two questions are duplicates.\n",
    "\n",
    "    Args:\n",
    "        question1 (str): First question.\n",
    "        question2 (str): Second question.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the questions are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # use `nltk` word tokenize function to tokenize\n",
    "    q1 = nltk.word_tokenize(question1)  # tokenize\n",
    "    q2 = nltk.word_tokenize(question2)  # tokenize\n",
    "    Q1, Q2 = [], []\n",
    "    for word in q1:  # encode q1\n",
    "        # increment by checking the 'word' index in `vocab`\n",
    "        Q1 += [vocab[word]]\n",
    "    for word in q2:  # encode q2\n",
    "        # increment by checking the 'word' index in `vocab`\n",
    "        Q2 += [vocab[word]]\n",
    "        \n",
    "    # Call the data generator (built in Ex 01) using next()\n",
    "    # pass [Q1] & [Q2] as Q1 & Q2 arguments of the data generator. Set batch size as 1\n",
    "    # Hint: use `vocab['<PAD>']` for the `pad` argument of the data generator\n",
    "    Q1, Q2 = next(data_generator(\n",
    "            [Q1], [Q2], 1, vocab['<PAD>']))\n",
    "    # Call the model\n",
    "    v1, v2 = model((Q1, Q2))\n",
    "    # take dot product to compute cos similarity of each pair of entries, v1, v2\n",
    "    # don't forget to transpose the second argument\n",
    "    d = np.dot(v1[0], v2[0].T)\n",
    "    # is d greater than the threshold?\n",
    "    res = d > threshold\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Q1  = \", Q1, \"\\nQ2  = \", Q2)\n",
    "        print(\"d   = \", d)\n",
    "        print(\"res = \", res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64db48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try with your own questions\n",
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, vocab, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try with your own questions\n",
    "question1 = \"Do they enjoy eating the dessert?\"\n",
    "question2 = \"Do they like hiking in the desert?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, vocab, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c0ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
