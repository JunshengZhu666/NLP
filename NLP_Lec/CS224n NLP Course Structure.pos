{"diagram":{"image":{"height":200,"pngdata":"iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAYAAACtWK6eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAsUlEQVR4nO3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB8GXHmAAFMgHIEAAAAAElFTkSuQmCC","width":200,"y":0,"x":0},"elements":{"id":"root","watermark":"","title":"CS224n NLP Course Structure","leftChildren":[{"id":"8ff438585257","title":"PART_FIVE","lineStyle":{"randomLineColor":"#80BA4C"},"children":[{"id":"60add78c9789","title":"Lec_10: Transformers &amp; Pretraining","children":[{"id":"d8a734943d3e","title":"subword modeling","children":[{"id":"1bd5f07b74ac","title":"Use commen word as subword vocabulary","children":[],"parent":"d8a734943d3e"},{"id":"dfa961b4ed67","title":"Split the rare word","parent":"d8a734943d3e","children":[]}],"parent":"60add78c9789"},{"id":"b25f074988cd","title":"Issues:","parent":"60add78c9789","children":[{"id":"75ccbcbe80eb","title":"The meaning of a word is contextual","parent":"b25f074988cd","children":[{"id":"1a2fe4079359","title":"Methods: Use pretraining to learn general things, and then finetune the model&nbsp;","parent":"75ccbcbe80eb","children":[]},{"id":"846dd5e35bdb","title":"Pretraining is like teaching by 'Cloze test'","children":[],"parent":"75ccbcbe80eb"}]}]},{"id":"9aa21e4e07cf","title":"Pretrianning in three ways","children":[{"id":"8f6f5cfab15e","title":"Encoders(can't use language model cause it has bi-directions);","children":[{"id":"76a98d735367","title":"Methods: Mask and learn","parent":"8f6f5cfab15e","children":[{"id":"c257dafe91dc","title":"BERT(Bi-direction Encoder Representations from Transformers)","children":[],"parent":"76a98d735367"}]}],"parent":"9aa21e4e07cf"},{"id":"a815c4dd98cd","title":"Decoders;","parent":"9aa21e4e07cf","children":[{"id":"01f6bbe84068","title":"Pretrian like language model, finetune on a last word classifer","parent":"a815c4dd98cd","children":[{"id":"13520500dc26","title":"Generative Pretrained Transformer (GPT) ","parent":"01f6bbe84068","children":[]}]}]},{"id":"3dfe061353ca","title":"Encoder-Decoder(Could be used in QA)","parent":"9aa21e4e07cf","children":[{"id":"e55fa7af7882","title":"Use span corruption","parent":"3dfe061353ca","children":[{"id":"5b8d0fbfc769","title":"T5","parent":"e55fa7af7882","children":[],"image":{"w":495,"h":212,"url":"http://cdn.processon.com/60b0924be0b34d38419044d6?e=1622188123&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:PNdbEVaQxpVv4fHUxWU-VA9gelc="}}]}]}],"parent":"60add78c9789"}],"parent":"8ff438585257"},{"id":"9f95ce3e9c23","title":"Lec_11: Question Answering&nbsp;","children":[{"id":"1d17231c823c","title":"","children":[{"id":"2290484e9643","title":"Evaluations","children":[{"id":"bb87465916f0","title":"Exact match","parent":"2290484e9643","children":[]},{"id":"b37949f92694","title":"F1&nbsp; partial credit","parent":"2290484e9643","children":[]}],"parent":"1d17231c823c"},{"id":"a4462bf39cab","title":"ModelsReading Comprehension(over text）:","parent":"1d17231c823c","children":[{"id":"c7cfd4796f3f","title":"BiDAF(the Bidirectional Attention Flow model):&nbsp;","parent":"a4462bf39cab","children":[{"id":"98bf93f9ecb9","title":"attention: find most relevant words to Question","parent":"c7cfd4796f3f","children":[]},{"id":"aa019388015d","title":"output layer: trian two classifer to predict the 'start' and 'end' words","parent":"c7cfd4796f3f","children":[]}]},{"id":"4e60238baaf7","title":"BERT for QA","children":[],"parent":"a4462bf39cab"},{"id":"383aa09565ee","title":"SpanBERT:&nbsp;","children":[{"id":"ef714d7e9244","title":"Open-domain QA(over documents)","children":[],"parent":"383aa09565ee"}],"parent":"a4462bf39cab"}]},{"id":"d6fbafa3b686","title":"Retriever-reader framework","children":[{"id":"f26440bd7974","title":"File Retriever (could be trianed too)","children":[],"parent":"d6fbafa3b686"},{"id":"1da269ad89c0","title":"Text reader","children":[],"parent":"d6fbafa3b686"}],"parent":"1d17231c823c"}],"parent":"9f95ce3e9c23"}],"parent":"8ff438585257"},{"id":"ca3deab43712","title":"Lec_12: Nutural Language Generation","children":[{"id":"1723123c84b7","title":"Trianing: Teacher Forcing (Maximum likeliood）","children":[],"parent":"ca3deab43712"},{"id":"a4975f3113e2","title":"Decoding (need Diversity)：","children":[{"id":"13fe0f4b41ab","title":"Top-p random sampling (sampling the tokens using probability mass)","children":[],"parent":"a4975f3113e2"},{"id":"6c14b6a7c8e5","title":"Decoding improving: backprop, re-ranking ...","parent":"a4975f3113e2","children":[]}],"parent":"ca3deab43712"},{"id":"1799c912d5e4","title":"Less diversity problem using EM methods","parent":"ca3deab43712","children":[{"id":"06a7ecd7e715","title":"Unlikelihood Training","parent":"1799c912d5e4","children":[{"id":"d5c2dcae13f3","title":"add undesired tokens 'C'","parent":"06a7ecd7e715","children":[]}]}]},{"id":"8d46c47a073a","title":"Evaluation","children":[{"id":"a60d68440a1c","title":"Content overlap","children":[{"id":"6ff3efe7e129","title":"N-gram overlap, semantic overlap","children":[],"parent":"a60d68440a1c"}],"parent":"8d46c47a073a"},{"id":"2e4fa26e1d8b","title":"Model-based&nbsp;","children":[{"id":"3324780cbed0","title":"sentence movers, vector similarity","children":[],"parent":"2e4fa26e1d8b"}],"parent":"8d46c47a073a"},{"id":"2b18045e16c0","title":"Gold standard","children":[],"parent":"8d46c47a073a"}],"parent":"ca3deab43712"}],"parent":"8ff438585257"},{"id":"cd98b6ae7785","title":"Ass05: Transformers (Not Done）","children":[{"id":"5a6db99c60e4","title":"Maths: self-attention, multi-head attention","parent":"cd98b6ae7785","children":[]},{"id":"29e20f724233","title":"codes: pretraining to learn facts, finetuning to get facts","parent":"cd98b6ae7785","children":[]}],"parent":"8ff438585257"}],"parent":"root"},{"id":"723e1d40a92c","title":"PART_SIX","lineStyle":{"randomLineColor":"#FD5155"},"children":[{"id":"24c21b525ed7","title":"Lec_13: Reference &amp; Conreference","parent":"723e1d40a92c","children":[{"id":"eddbd7058270","title":"Goal: Indentity all mentions that refer to the same entity)","image":{"w":341,"url":"http://cdn.processon.com/60b43453e0b34d2a3df67bc5?e=1622426211&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:U9XkYEfM7B4eW9EExvpQvjstyGw=","h":195},"parent":"24c21b525ed7","children":[]},{"id":"5963fc5fc5c5","title":"Methods","parent":"24c21b525ed7","children":[{"id":"041cb5ab5d13","title":"Detect","parent":"5963fc5fc5c5","children":[{"id":"e54379bf5a90","title":"POS: pronouns","parent":"041cb5ab5d13","children":[]},{"id":"c38e4ebe14b3","title":"NER: named entities","parent":"041cb5ab5d13","children":[]},{"id":"9c5001d1cbb7","title":"Parser: noun phrasers","parent":"041cb5ab5d13","children":[]}]},{"id":"32f46edf52f6","title":"Resolve","parent":"5963fc5fc5c5","children":[{"id":"a55e8da8c40a","title":"Ruled-based","children":[],"parent":"32f46edf52f6"},{"id":"8f9e31516152","title":"Mention Pair (trian a binary classifer, can't handle long&nbsp; distance) &amp;&nbsp;Mention Ranking","children":[],"parent":"32f46edf52f6"},{"id":"f607cd644917","title":"Neural Model (CNN + Bi-LSTM, BERT-based )","image":{"w":512,"url":"http://cdn.processon.com/60b436a8e401fd7a6b6eb2b8?e=1622426808&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:mSU8AYPf0guanwQJerD6Rj7nBH8=","h":66},"children":[],"parent":"32f46edf52f6"}]}]},{"id":"23417898fc0b","title":"Evaluation: like measuring the clustering","children":[],"parent":"24c21b525ed7"}]},{"id":"e829153fc897","title":"Lec_14: T5 &amp; Large Language model","parent":"723e1d40a92c","children":[{"id":"86c136e5c943","title":"T5","image":{"w":551,"url":"http://cdn.processon.com/60b4557e5653bb3c7e671e39?e=1622434702&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:XgTX-_0qWCnlQ9FcONvd9nsmn04=","h":224},"children":[],"parent":"e829153fc897"}]},{"id":"82d6e83eec0f","title":"Lec_15: Integrating Knowledge !!!","image":{"w":459,"url":"http://cdn.processon.com/60b43f32e401fd7a6b6ecbe4?e=1622428994&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:7ROgiv2dyicIJwyuh7bq2X_uCvk=","h":261},"parent":"723e1d40a92c","children":[{"id":"2a05a26e6689","title":"Method 1: add pretrained entity embeddings, link mentions in text to entities&nbsp; and learn a fusion layer","children":[{"id":"3b4f50e7ece8","title":"ERNIE: Enhanced Language Representation with Informative<br>Entities","parent":"2a05a26e6689","children":[]},{"id":"c6e8861245da","title":"KnowBERT:&nbsp; pretrain an integrated entity linker (EL) as an extension to BERT","parent":"2a05a26e6689","children":[]}],"parent":"82d6e83eec0f"},{"id":"7392ee758a98","title":"Method 2: use external memory, like knowledge graph triples","children":[{"id":"c9a59a08ce71","title":"KGLM: Using Knowledge-Graphs for Fact-Aware<br>Language Modeling. Condition the language model on a knowledge graph (KG) ","children":[],"parent":"7392ee758a98"},{"id":"bd188aa385b3","title":"KNN-LM:&nbsp; learning similarities between text sequences is easier than predicting the<br>next word","children":[],"parent":"7392ee758a98"}],"parent":"82d6e83eec0f"},{"id":"823c1b5d9b63","title":"Method 3: mask the data to introduce the addtional trianing tasks&nbsp;","children":[{"id":"94d5c9dcb9e6","title":"WKLM:&nbsp;Weakly Supervised KnowledgePretrained Language Model (To distinguish between true and false knowledge)","children":[],"parent":"823c1b5d9b63"}],"parent":"82d6e83eec0f"},{"id":"a4e76c2d603b","title":"Evaluation:&nbsp;","parent":"82d6e83eec0f","children":[{"id":"d5304bdc0fa2","title":"LAMA(LAnguage Model Analysis) Probe: test by manually constructed cloze statement","parent":"a4e76c2d603b","children":[]},{"id":"8cd4fee18524","title":"LAMA-UnHelpful Nanes: use only relational knowledge","parent":"a4e76c2d603b","children":[]}]}]},{"id":"b234c9c74dde","title":"Ass_Final_Project (Not Done)","parent":"723e1d40a92c","children":[]}],"parent":"root"},{"id":"57152537dc1d","title":"PART_SEVEN","lineStyle":{"randomLineColor":"#0F80C4"},"parent":"root","children":[{"id":"ac1850f92277","title":"Lec_16: Ethical consideration","parent":"57152537dc1d","children":[]},{"id":"ccc8ef68d9b2","title":"Lec_17: Model Analysis","image":{"w":416,"url":"http://cdn.processon.com/60b452365653bb3c7e670873?e=1622433862&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:fPf_mpBRzNJ8moMhBlPH21jJcuA=","h":271},"parent":"57152537dc1d","children":[{"id":"ad5d30461efe","title":"Prediction explanations: what in the input led to this output?","parent":"ccc8ef68d9b2","children":[{"id":"6b2aa8bd2b24","title":"Saliency maps: a score for each input word indicating its importance to the model’s prediction","parent":"ad5d30461efe","children":[]}]},{"id":"cb8eb93a9091","title":"Probing: supervised analysis of neural networks (Across a wide range of linguistic properties, the middle layers of BERT yield the best<br>probing accuracies.)","parent":"ccc8ef68d9b2","children":[]},{"id":"70fcb0466fc3","title":"Ablation analysis: do we need all these attention heads?","children":[],"parent":"ccc8ef68d9b2"}]},{"id":"021b805cdd21","title":"Lec_18: The future of NLP","image":{"w":490,"url":"http://cdn.processon.com/60b452d37d9c08055f33cc46?e=1622434019&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:Z4vhh28Bt7hBGg_h8EpvU8A2W6A=","h":197},"parent":"57152537dc1d","children":[{"id":"d3719486c221","title":"Large Language Models and GPT-3","children":[{"id":"93be027db35d","title":"Better than other models at language modeling and related tasks such as story<br>completion","children":[],"parent":"d3719486c221"},{"id":"93dd7bd34d61","title":"Flexible “in-context” learning","children":[],"parent":"d3719486c221"}],"parent":"021b805cdd21"},{"id":"b73a97fb799d","title":"Compositional Representations and Systematic Generalization","children":[{"id":"1a179edf03b6","title":"TRE [Andreas 2019]: Compositionality of representations is about how<br>well the representation approximates an explicitly homomorphic<br>function in a learnt representation space","children":[],"parent":"b73a97fb799d"},{"id":"461a9c33649d","title":"Pre-training helps for compositional generalization, but<br>doesnt solve it","children":[],"parent":"b73a97fb799d"}],"parent":"021b805cdd21"},{"id":"ec6c565bf02c","title":"Improving how we evaluate models in NLP","image":{"w":414,"url":"http://cdn.processon.com/60b459797d9c08055f33f4a9?e=1622435721&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:vdy9oH4WK5MPLTxzBgU5Ve9zpMo=","h":178},"children":[],"parent":"021b805cdd21"},{"id":"894c28f10385","title":"","image":{"w":488,"url":"http://cdn.processon.com/60b45aef7d9c08055f33fc39?e=1622436095&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:RX6qFvb9lxIQbCEx-04lepffKwA=","h":38},"children":[],"parent":"021b805cdd21"}]}]}],"structure":"mind_free","root":true,"theme":"delicate_caihong","children":[{"id":"3085b9d1fe4d","title":"PART_ONE","lineStyle":{"randomLineColor":"#DD489D"},"children":[{"id":"64fb42ed08c4","title":"Lec_1: Word Vectors","children":[{"id":"65a94f3a8c8b","title":"Word2Vec ( is a framework for learning word vectors)","parent":"64fb42ed08c4","children":[{"id":"24448fd39f91","title":"Use the similarity of the word vectors for c and o to calculate the probability of o given<br>c (or vice versa)","parent":"65a94f3a8c8b","children":[{"id":"91448cab4b62","title":"skip-gram (predict use center word)","parent":"24448fd39f91","children":[]},{"id":"d0513e478998","title":"bag-of-word (predict use outside word)","parent":"24448fd39f91","children":[]}]},{"id":"cfa4a9da36db","title":"","parent":"65a94f3a8c8b","children":[],"image":{"w":436,"url":"http://cdn.processon.com/60adf29d5653bb641179f8ff?e=1622016173&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:WW8tWqlxOA1-Dffx39uB7VZGtoc=","h":156}},{"id":"4184489362ca","title":"During trianing; use&nbsp;Stochastic gradient descent &amp; negetive sampling","children":[],"parent":"65a94f3a8c8b"}]}],"parent":"3085b9d1fe4d"},{"id":"30750ce1b3c0","title":"Lec_2: Word Vectors2","children":[{"id":"4cbc188f2076","title":"Use Windows and Co-occurrence matrix (for general topic)","children":[{"id":"2625f5deb6a7","title":"","children":[],"parent":"4cbc188f2076","image":{"w":296,"h":143,"url":"http://cdn.processon.com/60adf5161e08531e9c850d4c?e=1622016806&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:TdNwSjYC5aTezWmthDfdK5cl5to="}}],"parent":"30750ce1b3c0"},{"id":"e7beb6cd32f2","title":"Glove model&nbsp;","children":[],"parent":"30750ce1b3c0"},{"id":"261c946d93df","title":"Word vector evaluation:","children":[{"id":"8b53aaa61d34","title":"intrinsic: analogy &amp; hyperparameter&nbsp;","children":[],"parent":"261c946d93df"},{"id":"b6e3cce476b8","title":"extrinsic: subquential task","children":[],"parent":"261c946d93df"}],"parent":"30750ce1b3c0"}],"parent":"3085b9d1fe4d"},{"id":"3a8497f511af","title":"Python Review","children":[],"parent":"3085b9d1fe4d"},{"id":"09b046934376","title":"Ass01: Exploring Word Vector (Done)","children":[{"id":"1adcd649d7a6","title":"import: sys, pprint, matplotlib, numpy, sklearn, nltk","parent":"09b046934376","children":[]},{"id":"74537cf43b84","title":"1, Count_base word vector","parent":"09b046934376","children":[]},{"id":"cda69ddeb10b","title":"2, Prediction_base word vector","parent":"09b046934376","children":[]}],"parent":"3085b9d1fe4d"}],"parent":"root"},{"id":"84a3414674cb","title":"PART_TWO","lineStyle":{"randomLineColor":"#BE49C4"},"children":[{"id":"b7f42ccbe01a","title":"Lec_3: Nueral Network&nbsp; &amp; Backprop","children":[],"parent":"84a3414674cb"},{"id":"e993c518df0a","title":"Lec_4: Dependency Parsing","parent":"84a3414674cb","children":[{"id":"6efd9db98069","title":"linguistic structure","parent":"e993c518df0a","children":[{"id":"f5ae397c7ce4","title":"phrase structure grammar","parent":"6efd9db98069","children":[]},{"id":"76044d60cbcc","title":"context-free grammars (CFGs)","parent":"6efd9db98069","children":[{"id":"53775a8e0aa3","title":"(Dependency Structure)help extract semantic interpretation","children":[],"parent":"76044d60cbcc"},{"id":"3c4b792caf5f","title":"","image":{"w":314,"url":"http://cdn.processon.com/60adfc555653bb3cff553d9e?e=1622018662&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:O1y0ac_hn_xwu0uh9WpQhcgQzas=","h":191},"parent":"76044d60cbcc","children":[]}]}]},{"id":"0aa2360f958d","title":"types of ambiguites","children":[],"parent":"e993c518df0a"},{"id":"05c773089d5f","title":"annotated data (Universal Dependencies treebanks)","children":[],"parent":"e993c518df0a"},{"id":"b97b11337a43","title":"Dependency Parsing&nbsp;","children":[{"id":"a50ae8515ffd","title":"Projectivity( corresponding to a CFG tree)","children":[{"id":"fa540e4e2ae3","title":"Methods:","parent":"a50ae8515ffd","children":[{"id":"e959622023ee","title":"Dynamic programming","parent":"fa540e4e2ae3","children":[]},{"id":"58474f0bbde8","title":"Transition-based parsing(With Machine Learning)","parent":"fa540e4e2ae3","children":[{"id":"e1d7775a4043","title":"","image":{"w":190,"url":"http://cdn.processon.com/60adfe625653bb3cff554ef5?e=1622019186&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:df4NEvJ9D1f6AvW5mwudOO-IGSI=","h":162},"parent":"58474f0bbde8","children":[]},{"id":"960017e3b7cf","title":"Nueral Parser (word embedding + Part-of-Speech + dependency labels)","parent":"58474f0bbde8","children":[]}]},{"id":"52e915c98856","title":"Evaluation: Unlabeled Attachment Score (UAS) &amp; LAS","parent":"fa540e4e2ae3","children":[]}]}],"parent":"b97b11337a43"}],"parent":"e993c518df0a"}]},{"id":"c476056c4c33","title":"PyTorch Tutorial","parent":"84a3414674cb","children":[{"id":"133640e5bed8","title":"Intro","parent":"c476056c4c33","children":[{"id":"584fe7568e04","title":"tensor","parent":"133640e5bed8","children":[{"id":"73cb95438cde","title":"build tensor","parent":"584fe7568e04","children":[{"id":"2a2fd66385c7","title":"torch.ones_like(old_tensor)","parent":"73cb95438cde","children":[]},{"id":"1bcb918c5e4e","title":"torch.randn_like(old_tensor)","parent":"73cb95438cde","children":[]},{"id":"5fd5d22aa0e9","title":"torch.arange()","parent":"73cb95438cde","children":[]}]},{"id":"ab74212d0c8c","title":"tensor properities","parent":"584fe7568e04","children":[{"id":"d8b1f363fbae","title":"type(), shape(), size(), view(), squeeze()","children":[],"parent":"ab74212d0c8c"}]},{"id":"329274359724","title":"tensor indexing&nbsp;","parent":"584fe7568e04","children":[]},{"id":"67fa304cca8c","title":"tensor operations","parent":"584fe7568e04","children":[{"id":"e99e5234dabb","title":"torch.cat()","parent":"67fa304cca8c","children":[]}]}]},{"id":"73dccb87b073","title":"Autograd","parent":"133640e5bed8","children":[]}]},{"id":"7251076d8ed6","title":"Nueral Network","parent":"c476056c4c33","children":[{"id":"2b1c4013d066","title":"Linear: nn.Linear()","parent":"7251076d8ed6","children":[]},{"id":"1fe54ce95190","title":"Other layers:&nbsp;nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm1d, nn.BatchNorm2d","parent":"7251076d8ed6","children":[]},{"id":"eb97a69d5661","title":"Activation Layers: nn.Relu(), nn.Sigmoid()","parent":"7251076d8ed6","children":[]},{"id":"687999c1bf39","title":"Put together: nn.Sequential(), nn.Module()","parent":"7251076d8ed6","children":[]},{"id":"0305c82e4b65","title":"Optimization:&nbsp; torch.optim (optim.SGD, opti.Adam)","children":[],"parent":"7251076d8ed6"}]},{"id":"71dedec542fe","title":"Demo: Word Window Classification","children":[],"parent":"c476056c4c33"}]},{"id":"8cf22ed691fe","title":"Ass02: Word2Vec (Done)","parent":"84a3414674cb","children":[{"id":"f1d265097413","title":"Skip-gram Word2Vec: nagetive sampling + SGD","children":[],"parent":"8cf22ed691fe"}]}],"parent":"root"},{"id":"f93eeddc8415","title":"PART_THREE","lineStyle":{"randomLineColor":"#3D5EC2"},"children":[{"id":"beb55d216b57","title":"Lec_5: RNN","children":[{"id":"91c637a3642b","title":"Regularization: Dropout, Vectorization,&nbsp;","children":[],"parent":"beb55d216b57"},{"id":"4a509b2f09c6","title":"Parameter Opima: Adam, Adagrad","children":[],"parent":"beb55d216b57"},{"id":"9a28ec5993ba","title":"Language Modeling (To predict the next words)","image":{"w":402,"url":"http://cdn.processon.com/60ae10e06376893238ddf01a?e=1622023920&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:A6ZaiP8Orhxgd92mA5OrqnuQ2LE=","h":237},"children":[{"id":"b6b46c8c88a7","title":"N-gram model","image":{"w":374,"url":"http://cdn.processon.com/60ae0fbae401fd06e1c216da?e=1622023626&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:qn6ZAX9BhIqEnx3gbnAayfr-t6E=","h":106},"children":[],"parent":"9a28ec5993ba"},{"id":"b3155bce3f9b","title":"Evaluation: Perplexity (inverse prop of corpus) The Lower the better","children":[],"parent":"9a28ec5993ba"}],"parent":"beb55d216b57"}],"parent":"f93eeddc8415"},{"id":"f1d055c8ade6","title":"Lec_6: Vanishing Gradient &amp; Seq2Seq","image":{"w":404,"url":"http://cdn.processon.com/60ae150ee401fd06e1c24273?e=1622024991&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:2dT__2x9yPp8SJORgfc7nuXwwW0=","h":194},"children":[{"id":"dd0ae5fda1b6","title":"LSTM","children":[{"id":"723056ae52db","title":"Hidden state h(t) &amp; Cell state c(t)","children":[],"parent":"dd0ae5fda1b6"}],"parent":"f1d055c8ade6"},{"id":"90517b779943","title":"Other way: direct(skip) connections","children":[],"parent":"f1d055c8ade6"}],"parent":"f93eeddc8415"},{"id":"bbeb6c863c91","title":"Ass03: Dependency Parsing &amp; NN (Done)","children":[{"id":"a86dfac2f1f7","title":"Adam &amp; Dropout Algorithm","children":[],"parent":"bbeb6c863c91"},{"id":"86398526ca30","title":"Neural Transition-Based Dependency Parsing","children":[],"parent":"bbeb6c863c91"}],"parent":"f93eeddc8415"}],"parent":"root"},{"id":"4def3b892768","title":"PART_FOUR","lineStyle":{"randomLineColor":"#FCB52A"},"children":[{"id":"80d6f74fb8cb","title":"Lec_7: Attention &amp; Subword models","children":[{"id":"1e9b448dff25","title":"1, Statistical Machine Translation&nbsp;","children":[],"parent":"80d6f74fb8cb"},{"id":"9977236e2fd3","title":"2, Neural Machine Translation(Seq2Seq)","children":[{"id":"35aca90529d3","title":"&nbsp;Beam search Decoding(like a tree with fixed nodes, need to normalize)","children":[],"parent":"9977236e2fd3"}],"parent":"80d6f74fb8cb"},{"id":"92da34442eff","title":"3, Evaluation: BLEU (Bilingual Evaluation Understudy)","children":[{"id":"e4e2a4330efc","title":"Computes a similarity score based on n-gram precision &amp; too short penalty","parent":"92da34442eff","children":[]}],"parent":"80d6f74fb8cb"},{"id":"6f48651066b9","title":"4, Attention (query the encoder from decoder)","image":{"w":368,"h":213,"url":"http://cdn.processon.com/60ae1b9b07912962458d777b?e=1622026667&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:qJ3EZDoROCBU-gzq38j3fdbKFpg="},"children":[{"id":"8f913fe40a4d","title":"Bottleneck problem&nbsp;","children":[],"parent":"6f48651066b9"}],"parent":"80d6f74fb8cb"}],"parent":"4def3b892768"},{"id":"ef2d9e070a34","title":"Lec_8: Projects Tips","children":[{"id":"46efd4edb7be","title":"Advice &amp; NLP data resources websites","children":[],"parent":"ef2d9e070a34"}],"parent":"4def3b892768"},{"id":"f9d742756aec","title":"Lec_9: Transformers","children":[{"id":"e917eeb5ec8e","title":"issue:linear interaction distance","children":[{"id":"0786a49bfc54","title":"self-attention: queries - q, keys - k, values - v","children":[],"parent":"e917eeb5ec8e"},{"id":"fbfe1502724d","title":"aggregated local words window models","children":[],"parent":"e917eeb5ec8e"}],"parent":"f9d742756aec"},{"id":"97cebfb404f3","title":"Transformers","parent":"f9d742756aec","children":[{"id":"923aaac59c4d","title":"self-attention (extras: add position representations, mask the future words","parent":"97cebfb404f3","children":[],"image":{"w":433,"h":103,"url":"http://cdn.processon.com/60ae220307912962458d931a?e=1622028307&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:X77WylSOyuE2BwDXKeFfM_1aOnc="}},{"id":"87e79a7d1dc1","title":"multi-head attention: want to focus on different word j for word i&nbsp;","parent":"97cebfb404f3","children":[]},{"id":"1e9a127db53b","title":"trianing tricks: residual connections, layer normalization, scalered dot product","parent":"97cebfb404f3","children":[]}],"image":{"w":443,"h":213,"url":"http://cdn.processon.com/60ae22c8f346fb715d515888?e=1622028504&amp;token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:8It0uXVHBiJrYr95ET13axVETGo="}}],"parent":"4def3b892768"},{"id":"8a38ca717b96","title":"Ass04: Machine Translation (Not done)","children":[{"id":"a46c70a1d14d","title":"1, Model: RNNs (Bi-LSTM Encoder + LSTM Decoder)","parent":"8a38ca717b96","children":[]},{"id":"49583544b1de","title":"2, Model analysis: BLEU","children":[],"parent":"8a38ca717b96"}],"parent":"4def3b892768"}],"parent":"root"}],"note":""}},"meta":{"id":"60adbae51e08531e9c83f11a","member":"608a7dfaf346fb7dd3921bae","exportTime":"2021-05-31 11:42:32","diagramInfo":{"category":"mind_free","title":"CS224n NLP Course Structure","created":"2021-05-26 11:05:10","creator":"608a7dfaf346fb7dd3921bae","modified":"2021-05-31 11:41:41"},"type":"ProcessOn Schema File","version":"1.0"}}